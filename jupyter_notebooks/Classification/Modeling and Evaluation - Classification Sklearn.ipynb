{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Classification Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a classification model to predict if tomorrow will rain or not.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Classifier model\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install ppscore==1.2.0\n",
        "! pip install pingouin==0.3.12\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "# import uuid\n",
        "# file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "# with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "# print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "# print(\"\\n\\n\")\n",
        "# os.remove(f\"{file_name}.txt\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def ReplaceTargetLabel(df):\n",
        "  df['RainTomorrow'] = df['RainTomorrow'].replace({\"Yes\":1,\"No\":0})\n",
        "  return df\n",
        "\n",
        "df = (pd.read_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\")\n",
        "      .drop(labels=['RainfallTomorrow'],axis=1)  # target variable for classifier\n",
        "      .dropna(subset=['RainTomorrow'])   # drop missing data from target RainTomorrow\n",
        "      .dropna(subset=['RainfallToday', 'RainToday']) #    ????????\n",
        "      .pipe(ReplaceTargetLabel)\n",
        "      .reset_index(drop=True)\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# ML Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kD7PZ5kZkBT"
      },
      "source": [
        "## Custom transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96j6zLKZz7N"
      },
      "source": [
        "  * convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "  * get Get Day, Month, Year, Weekday, IsWeekend from Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTrXFaWSIU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "class ConvertToCategorical(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variables=None):\n",
        "      if not isinstance(variables, list):\n",
        "          self.variables = [variables]\n",
        "      else:\n",
        "          self.variables = variables\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      for feature in self.variables:\n",
        "          X[feature] = X[feature].astype('object')\n",
        "\n",
        "      return X\n",
        "\n",
        "\n",
        "# Get Day, Month, Year, Weekday, IsWeekend from Date\n",
        "class GetFeaturesFromDate(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variable=None):\n",
        "      self.variable = variable\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      X[self.variable] = pd.to_datetime(X[self.variable])\n",
        "      X['Day'] = X[self.variable].dt.day\n",
        "      X['Month'] = X[self.variable].dt.month\n",
        "      X['Year'] = X[self.variable].dt.year\n",
        "      X['WeekDay']= X[self.variable].dt.weekday\n",
        "      X['IsWeekend'] = X['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "      return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCsXhBYVBJw"
      },
      "source": [
        "## Load estimators needed at Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk1RSVSYVBWw"
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import DropMissingData\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "from feature_engine.encoding import RareLabelEncoder,OrdinalEncoder\n",
        "\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.svm import SVC \n",
        "from sklearn.svm import LinearSVC \n",
        "from sklearn.svm import NuSVC \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.naive_bayes import ComplementNB \n",
        "from sklearn.naive_bayes import BernoulliNB \n",
        "from sklearn.naive_bayes import CategoricalNB \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "# # Ensemble methods - goal is to combine the predictions of several base estimators\n",
        "# # in order to improve generalizability / robustness over a single estimator\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## Data Cleaninig And Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      ###### Data Cleaning ######\n",
        "      (\"ConvertToCategorical\",ConvertToCategorical(variables = ['Cloud9am','Cloud3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"GetFeaturesFromDate\",GetFeaturesFromDate(variable = 'Date')\n",
        "      ),\n",
        "       \n",
        "      \n",
        "      # drop rows when load data, not in the pipeline\n",
        "      # if it drops on the pipeline while training, it will not remove the respective rows from the target.\n",
        "      # (\"DropMissingData\",DropMissingData(variables = ['RainfallToday', 'RainToday'])\n",
        "      # ),\n",
        "\n",
        "      (\"CategoricalImputer\",CategoricalImputer(variables = ['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm'],\n",
        "                                               imputation_method='missing',fill_value='Missing')\n",
        "      ),\n",
        "\n",
        "      (\"MedianImputer\",MeanMedianImputer(imputation_method='median',\n",
        "                                         variables=['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                                                  'Humidity3pm', 'Temp3pm', 'WindSpeed3pm',\n",
        "                                                  'Humidity9am','WindSpeed9am','Temp9am',\n",
        "                                                  'MaxTemp','RainfallToday','Sunshine']\n",
        "                                          )\n",
        "      ),\n",
        "\n",
        "      (\"MeanImputer\",MeanMedianImputer(imputation_method='mean',variables=['MinTemp'])\n",
        "      ),\n",
        "       \n",
        "      (\"DropFeatures\",DropFeatures(features_to_drop = ['Evaporation','Cloud9am','Date',\n",
        "                                                       'Sunshine','RainfallToday'\n",
        "                                                       ])\n",
        "      ),  \n",
        "\n",
        "      ###### Feature Engineering ######\n",
        "       \n",
        "      (\"RareLabelEncoder_tol5\",RareLabelEncoder(tol=0.05, n_categories=2, variables=['WindDir3pm'])\n",
        "      ),\n",
        "       \n",
        "      (\"RareLabelEncoder_tol7\",RareLabelEncoder(tol=0.06, n_categories=2, variables=['State'])\n",
        "      ),\n",
        "       \n",
        "\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary', \n",
        "                                                  variables = ['Location','WindGustDir','WindDir9am',\n",
        "                                                               'WindDir3pm','State','Cloud3pm','RainToday'])\n",
        "      ),\n",
        "\n",
        "\n",
        "      # (\"Winsorizer_iqr\",Winsorizer(capping_method='iqr',tail='both', fold=3,variables = ['RainfallToday'])\n",
        "      # ),\n",
        "\n",
        "\n",
        "      (\"PowerTransformer\",PowerTransformer(variables = ['WindSpeed3pm','Humidity3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"YeoJohnsonTransformer\",YeoJohnsonTransformer(variables=['WindGustSpeed','WindSpeed9am','Humidity9am',\n",
        "                                                                # 'RainfallToday'\n",
        "                                                                ])\n",
        "      ),\n",
        "\n",
        "      (\"EqualFrequencyDiscretiser\",EqualFrequencyDiscretiser(q=5,variables = ['Latitude','Longitude'])\n",
        "      ),\n",
        "       \n",
        "\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDcH5Wn5UIXT"
      },
      "source": [
        "## Classifier - Single Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebBu5hWDULkn"
      },
      "source": [
        "def PipelineClassifier():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "  pipe.steps.append([\n",
        "                    \"scaler\",StandardScaler()\n",
        "                    ])\n",
        "\n",
        "  pipe.steps.append([\n",
        "                     \"feat_selection\",SelectFromModel(XGBClassifier(random_state=config.RANDOM_STATE))\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"model\",XGBClassifier(random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  return pipe\n",
        "\n",
        "\n",
        "\n",
        "# PipelineClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7BXNYMULrf"
      },
      "source": [
        "## Hyperparameter Optmization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Pipeline Optmization: Add Feature Scaling, Feature Selection and Model\n",
        "def PipelineOptmization(model):\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "  pipe.steps.append([\"scaler\", StandardScaler()])\n",
        "  pipe.steps.append([\"feat_selection\", SelectFromModel(model)])\n",
        "  pipe.steps.append([\"model\", model])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "# Custom Class for hyperparameter Optmization\n",
        "class HyperparameterOptmizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model=  PipelineOptmization(self.models[key])\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, \n",
        "                              # refit=refit,\n",
        "                              # return_train_score=True\n",
        "                              )\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs    \n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            # print(k)\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vDjhS40UN_s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-WuiIvawEm"
      },
      "source": [
        "## Date before hitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWYdGoVMdazt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr_pps(df,threshold):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,12))\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                        mask=mask,cmap='rocket_r', annot_kws={\"size\": 8})\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr_pps(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr_pps(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "  heatmap_corr_pps(df=pps_matrix,threshold=PPS_Threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMwlRwlhayKq"
      },
      "source": [
        "FeaturesTrainSet = df.copy().drop(['RainTomorrow'],axis=1)\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(FeaturesTrainSet)\n",
        "                                        .columns)\n",
        "\n",
        "\n",
        "pipeline_before_model = Pipeline(PipelineClassifier().steps[:-2])\n",
        "df_before_hitting_model = pd.DataFrame(data = pipeline_before_model.fit_transform(FeaturesTrainSet,df['RainTomorrow']),\n",
        "                                       columns = columns_after_data_cleaning_feat_eng\n",
        "                                       )\n",
        "\n",
        "# df_before_hitting_model = pd.concat([df_before_hitting_model,df['RainTomorrow']],axis=1)\n",
        "\n",
        "print(df_before_hitting_model.shape)\n",
        "df_before_hitting_model.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6MYVZrYcwph"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df=df_before_hitting_model, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sT5XfcVdk1V"
      },
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_before_hitting_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05X9aVIkdlUj"
      },
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold=0.6, PPS_Threshold=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "* Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD6B3CuhiDMT"
      },
      "source": [
        "* Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFzP2iGiIk1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['RainTomorrow'],axis=1),\n",
        "                                    df['RainTomorrow'],\n",
        "                                    test_size = config.TEST_SIZE,\n",
        "                                    random_state = config.RANDOM_STATE,\n",
        "                                    stratify= df['RainTomorrow']\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uQR5OiHGBFB"
      },
      "source": [
        "* Target Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOkkeYh2GBM9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUmGuf3TUkfE"
      },
      "source": [
        "## Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-mLYa7Usch"
      },
      "source": [
        "### Quick Search using model's default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXpLRzZUkoH"
      },
      "source": [
        "models_quick_search = {\n",
        "    # 'RidgeClassifier':RidgeClassifier(config.RANDOM_STATE),\n",
        "    # \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE),\n",
        "    \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"RandomForestClassifier\":RandomForestClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"AdaBoostClassifier\":AdaBoostClassifier(random_state=config.RANDOM_STATE),\n",
        "    # \"XGBClassifier\":XGBClassifier(random_state=config.RANDOM_STATE)\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    'RidgeClassifier': {},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"DecisionTreeClassifier\":{},\n",
        "    \"RandomForestClassifier\":{},\n",
        "    \"GradientBoostingClassifier\":{},\n",
        "    \"ExtraTreesClassifier\":{},\n",
        "    \"AdaBoostClassifier\":{},\n",
        "    \"XGBClassifier\":{},\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzyKHnhVVlMu"
      },
      "source": [
        "from sklearn.metrics import f1_score, make_scorer\n",
        "quick_search = HyperparameterOptmizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "quick_search.fit(X_train, y_train,\n",
        "                #  scoring =  make_scorer(f1_score , average='macro',pos_label=2),\n",
        "                 scoring='accuracy',\n",
        "                 n_jobs=-1, cv=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fL3YG8LVvOs"
      },
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='max_score')\n",
        "grid_search_summary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHovHEV6Vwsg"
      },
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu-Iu4p3VyFy"
      },
      "source": [
        "grid_search_pipelines[best_model].best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZtkABXdVzPJ"
      },
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVO0_T40V3lm"
      },
      "source": [
        "# after data cleaning, the feature space changes\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf['feat_selection'].get_support()].to_list()\n",
        "print(f\"* These are the {len(best_features)} most important features. \"\n",
        "      f\"The model was trained on them: \\n{best_features}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00Sa_FiUjNY"
      },
      "source": [
        "## Old Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLkUcA7riJD0"
      },
      "source": [
        "* Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imwlDdi6iJK-"
      },
      "source": [
        "pipeline_clf = PipelineClassifier()\n",
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-15-sWUST6XX"
      },
      "source": [
        "### GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoOsOsPbT6gN"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "_parameters = {\n",
        "    # 'model__C': [1,0.5,2],\n",
        "    # 'model__tol': [1e-4,1e-3,1e-5],\n",
        "    # 'model__learning_rate': [0.01,0.1,0.001],\n",
        "    'model__n_estimators':[100,50], # [100,200,50],\n",
        "    # 'model__max_depth': [None,3] # [None,3,10]\n",
        "}\n",
        "\n",
        "\n",
        "_pipe = GridSearchCV(\n",
        "\t\testimator = pipeline_clf,\n",
        "\t\tparam_grid = _parameters, \n",
        "\t\tcv=2,n_jobs=-1,verbose=2,\n",
        "    scoring = \"roc_auc\")\n",
        "_pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QPWZ6ajUhmf"
      },
      "source": [
        "pipeline_clf = _pipe.best_estimator_\n",
        "_pipe.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXodqMUHfYHd"
      },
      "source": [
        "pipeline_clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y412YjKxUg8_"
      },
      "source": [
        "# X_train.columns[best_clf_pipeline['feat_selection'].get_support()].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "### Fit Clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "pipeline_clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM0yjzb0RptS"
      },
      "source": [
        "pipeline_clf['feat_selection'].get_support()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Classifier Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gkzUT-xW3jx"
      },
      "source": [
        "## Custom Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myG6tDSGan4r"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "def PredictionEvaluation(X,y,pipeline):\n",
        "\n",
        "  prediction = pipeline.predict(X)\n",
        "\n",
        "  LabelsMap = {1:\"Yes\",0:\"No\"}\n",
        "  Map = list() \n",
        "  for key, value in LabelsMap.items():\n",
        "    Map.append( str(key) + \": \" + value)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y,prediction),\n",
        "        columns=[ [\"Actual \" + sub for sub in Map] ], \n",
        "        index = [ [\"Prediction \" + sub for sub in Map ]]\n",
        "        # index=['Prediction 0', 'Prediction 1']\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction),\"\\n\")\n",
        "\n",
        "\n",
        "  print('--- Area Under the Receiver Operating Characteristic Curve (ROC AUC)  ---')\n",
        "  print(roc_auc_score(y, prediction).round(3),\"\\n\\n\")\n",
        "\n",
        "\n",
        "def PerformanceTrainTestSet(X_train,y_train,X_test,y_test,pipeline):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  PredictionEvaluation(X_train,y_train,pipeline)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  PredictionEvaluation(X_test,y_test,pipeline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpUfEAGlW5aK"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-N7L4LFbEfL"
      },
      "source": [
        "PerformanceTrainTestSet(X_train,y_train,X_test,y_test,pipeline_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3Sds3BiKEm"
      },
      "source": [
        "X_test.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiKlI81PYnv0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}