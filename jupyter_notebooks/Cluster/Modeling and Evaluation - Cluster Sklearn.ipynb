{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a cluster model to group australian cities/states based on weather information\n",
        "* Understand profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster model\n",
        "* Classifier modeel to explain clusters\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "\n",
        "* how to translate cluster to map?\n",
        "  * dataset is time series, each row is a day for each city\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "! pip install yellowbrick==1.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "# import uuid\n",
        "# file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "# with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "# print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "# print(\"\\n\\n\")\n",
        "# os.remove(f\"{file_name}.txt\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\")\n",
        "df.drop(['RainTomorrow','RainfallTomorrow'],axis=1,inplace=True)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kD7PZ5kZkBT"
      },
      "source": [
        "## Custom transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96j6zLKZz7N"
      },
      "source": [
        "  * convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "  * get Get Day, Month, Year, Weekday, IsWeekend from Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTrXFaWSIU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "class ConvertToCategorical(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variables=None):\n",
        "      if not isinstance(variables, list):\n",
        "          self.variables = [variables]\n",
        "      else:\n",
        "          self.variables = variables\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      for feature in self.variables:\n",
        "          X[feature] = X[feature].astype('object')\n",
        "\n",
        "      return X\n",
        "\n",
        "\n",
        "# Get Day, Month, Year, Weekday, IsWeekend from Date\n",
        "class GetFeaturesFromDate(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variable=None):\n",
        "      self.variable = variable\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      X[self.variable] = pd.to_datetime(X[self.variable])\n",
        "      X['Day'] = X[self.variable].dt.day\n",
        "      X['Month'] = X[self.variable].dt.month\n",
        "      X['Year'] = X[self.variable].dt.year\n",
        "      X['WeekDay']= X[self.variable].dt.weekday\n",
        "      X['IsWeekend'] = X['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "      return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML Pipeline: Base, Cluster and ClfToExplainClusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import DropMissingData\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      ### Data Cleaning\n",
        "      (\"ConvertToCategorical\",ConvertToCategorical(variables = ['Cloud9am','Cloud3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"GetFeaturesFromDate\",GetFeaturesFromDate(variable= 'Date')\n",
        "      ),\n",
        "\n",
        "      (\"DropFeatures\",DropFeatures(features_to_drop = ['Sunshine','Evaporation','Cloud9am','Date'])\n",
        "      ),\n",
        "\n",
        "      (\"DropMissingData\",DropMissingData(variables =['RainfallToday', 'RainToday',\n",
        "                                                      # 'RainfallTomorrow','RainTomorrow'\n",
        "                                                      ])\n",
        "      ),\n",
        "\n",
        "      (\"CategoricalImputer\",CategoricalImputer(variables=['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm'],\n",
        "                                                imputation_method='missing',fill_value='Missing')\n",
        "      ),\n",
        "\n",
        "      (\"MedianImputer\",MeanMedianImputer(imputation_method='median',\n",
        "                                          variables=['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                                                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                                                    'WindSpeed9am','Temp9am','MaxTemp']\n",
        "                                          )\n",
        "      ),\n",
        "\n",
        "      (\"MeanImputer\",MeanMedianImputer(imputation_method='mean',variables=['MinTemp'])\n",
        "      ),\n",
        "\n",
        "      ### Feature Engineering\n",
        "\n",
        "      (\"Winsorizer_iqr\",Winsorizer(capping_method='iqr',tail='both', fold=3,variables = ['RainfallToday'])\n",
        "      ),\n",
        "\n",
        "\n",
        "      (\"PowerTransformer\",PowerTransformer(variables = ['WindSpeed3pm','Humidity3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"YeoJohnsonTransformer\",YeoJohnsonTransformer(variables=['RainfallToday','WindGustSpeed','WindSpeed9am','Humidity9am'])\n",
        "      ),\n",
        "\n",
        "      (\"EqualFrequencyDiscretiser\",EqualFrequencyDiscretiser(q=5,variables = ['Latitude','Longitude' ])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol5\",RareLabelEncoder(tol=0.05, n_categories=2, variables=['WindDir3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol7\",RareLabelEncoder(tol=0.06, n_categories=2, variables=['State'])\n",
        "      ),\n",
        "\n",
        "      (\"CountEncoder\",CountFrequencyEncoder(encoding_method='count',\n",
        "                                            variables = ['Location','WindGustDir','WindDir9am',\n",
        "                                                          'WindDir3pm','State','Cloud3pm',\n",
        "                                                          'RainToday'])##############\n",
        "      ),\n",
        "\n",
        "      # ### Feature Selection - Dimensionality Reduction    \n",
        "      # (\"PCA\",PCA(n_components=3,random_state=config.RANDOM_STATE)\n",
        "      # ),\n",
        "\n",
        "      # ### Feature Scaling\n",
        "      # (\"scaler\",StandardScaler()\n",
        "      # ),\n",
        "      \n",
        "      # ### Model\n",
        "      # (\"model\",KMeans(n_clusters=5,random_state=config.RANDOM_STATE)\n",
        "      # )\n",
        "\n",
        "    ]\n",
        "  )\n",
        "  return pipeline_base\n",
        "\n",
        "\n",
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "  pipe.steps.append([\n",
        "                     \"PCA\",PCA(n_components=3,random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"model\",KMeans(n_clusters=5,random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  return pipe\n",
        "\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "\n",
        "   pipe.steps.append([\n",
        "                     \"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=config.RANDOM_STATE))\n",
        "                     ])\n",
        "   \n",
        "   pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "   \n",
        "   pipe.steps.append([\n",
        "                     \"model\",GradientBoostingClassifier(random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "   return pipe\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItBO8zrl2LTs"
      },
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "# Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fnArcWET0D"
      },
      "source": [
        "* It needs the dataset after data cleaning and feature engineering\n",
        "  * That means you have to remove 3 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "source": [
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-3])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "df_pca.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "* Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "n_components = 3\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFC6gDrfq83G"
      },
      "source": [
        "* Heatmap: PCA components and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plQLu3Z3tE5Y"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df_comp = pd.DataFrame(pca.components_, columns=df_pca.columns)\n",
        "plt.figure(figsize=(20,5))\n",
        "sns.heatmap(df_comp,center=0,linewidths=.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "# Elbow Analysis and Quick Silhouete Visualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXb_H6iGx6U"
      },
      "source": [
        "* Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "source": [
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df)\n",
        "df_elbow.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KLQujEvgi"
      },
      "source": [
        "* Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(), k=(1,16))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "\n",
        "# 6 clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMPygnLNHDnV"
      },
      "source": [
        "* Quick Silhouete Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MjrBCqAGcbD"
      },
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 5\n",
        "visualizer = SilhouetteVisualizer(KMeans(n_clusters=n_clusters),colors='yellowbrick')\n",
        "visualizer.fit(df_elbow)\n",
        "visualizer.show()  \n",
        "\n",
        "# takes 9m28s when clusters=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGXCMGSjD7J6"
      },
      "source": [
        "# pca 4,  6 clusters, silhouete score 0.52\n",
        "# pca3, 5 clusers, silhoute 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "X = df.copy()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU1E4IdkyLd"
      },
      "source": [
        "* Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lx5EqalOtv"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk0uyz1lSIW"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJzUxAmkv07"
      },
      "source": [
        "* The goal is to merge cluster labels to our data.\n",
        "  * However,the pipeline dropped rows from ['RainfallToday', 'RainToday'].\n",
        "  * Before merging, we need to adjust it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUY6-_6A-rN"
      },
      "source": [
        "drop_imputer = DropMissingData(variables =['RainfallToday', 'RainToday'])\n",
        "drop_imputer.fit(X)\n",
        "X = drop_imputer.transform(X)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "* We add a column \"Cluster\" to the data and check clusters distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "X['Clusters'] = X['Clusters'].astype('object')\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OhXSh7536Ye"
      },
      "source": [
        "* Clusters don't look to be imbalanced\n",
        "* This is how our data look like from now\n",
        "  * Check the last column: Clusters\n",
        "  * Quick reminder: The data is unprocessed (data cleaning, feat eng)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "source": [
        "X.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Clusters Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4j4h-2Shlq4"
      },
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df)\n",
        "df_transformed.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqxawC8lq1R"
      },
      "source": [
        "from config import config\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "\n",
        "def EvaluateClusterSilhouette(X,Clusters):\n",
        "\n",
        "\tn_clusters = len(set(Clusters))\n",
        "\n",
        "\tprint(\" Silhouette plot for each cluster\")\n",
        "\tfig, (ax1) = plt.subplots(1, 1)\n",
        "\tfig.set_size_inches(18, 7)\n",
        "\tax1.set_xlim([-0.1, 1])\n",
        "\tax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\t\n",
        "\tsilhouette_avg = silhouette_score(X, cluster_labels,random_state=config.RANDOM_STATE)\n",
        "\tprint(\"* The silhouette average score is \",str(round(float(silhouette_avg),2)))\n",
        "\t# print(\n",
        "\t# \tf\"* Silhouette assesses consistency within clusters - \"\n",
        "\t# \tf\"[Link 1] (https://en.wikipedia.org/wiki/Silhouette_(clustering)) and \"\n",
        "\t# \tf\"[Link 2] (https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam) \")\n",
        "\n",
        "\n",
        "\tsample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\ty_lower = 10\n",
        "\tfor i in range(n_clusters):\n",
        "\t\tith_cluster_silhouette_values = \\\n",
        "\t\t\tsample_silhouette_values[cluster_labels == i]\n",
        "\t\tith_cluster_silhouette_values.sort()\n",
        "\t\tsize_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "\t\ty_upper = y_lower + size_cluster_i\n",
        "\t\tcolor = cm.nipy_spectral(float(i) / n_clusters)\n",
        "\t\tax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "\t\t\t\t\t\t\t0, ith_cluster_silhouette_values,\n",
        "\t\t\t\t\t\t\tfacecolor=color, edgecolor=color, alpha=0.7)\n",
        "\t\tax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\t\ty_lower = y_upper + 10\n",
        "\n",
        "\tax1.set_title(\"The silhouette plot for each cluster\")\n",
        "\tax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "\tax1.set_ylabel(\"Cluster label\")\n",
        "\tax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\tax1.set_yticks([])\n",
        "\tax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsrpxXqljJ-y"
      },
      "source": [
        "EvaluateClusterSilhouette(\n",
        "    X=df_transformed,\n",
        "    Clusters=X['Clusters'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlNCuj0JkIdd"
      },
      "source": [
        "# State and RainToday distribution per cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7MqATPNlsKk"
      },
      "source": [
        "variables_of_interest = ['State','RainToday']\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "for col in variables_of_interest:\n",
        "  df_stack = X.filter([col,'Clusters'],axis=1).groupby([col,'Clusters']).size().reset_index()\n",
        "  df_stack.rename({0:\"Count\"},axis=1,inplace=True)\n",
        "  fig = px.bar(df_stack, color = col, y = 'Count', x = 'Clusters', barmode = 'stack',width=None, height=400)\n",
        "  fig.update_xaxes(type='category',categoryorder='category ascending')\n",
        "  \n",
        "  print(f\"* Clusters per {col}\")\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "\n",
        "# 0 -  type of data that doenst happen in south of wales, victoria\n",
        "# 1 -  type of day that happens only in new south wales\n",
        "# 2 - type of day that happens every state: rain day\n",
        "# 3 - type of day that happens only in south of wales, victoria\n",
        "# 4 -  type of day that doesnt happens only in new south wales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "# Classifier to explain cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "* We need to find the most relevant variables, to define each cluster in terms of each relevant variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "source": [
        "df_clf = X.copy() #.sample(frac=0.1, random_state=config.RANDOM_STATE)\n",
        "df_clf['Clusters'] = df_clf['Clusters'].astype('int32')\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE,\n",
        "                                    stratify=df_clf['Clusters']\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "* Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Cjsn6l5cqF"
      },
      "source": [
        "* Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QPwTqa0OcY"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train,y_train)\n",
        "\n",
        "# do GridCV after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "* Check main features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIsagqVE1a4d"
      },
      "source": [
        "df_clf.columns[pipeline_clf_cluster['feat_selection'].get_support()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print( classification_report(y_train, pipeline_clf_cluster.predict(X_train)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "source": [
        "print( classification_report(y_test, pipeline_clf_cluster.predict(X_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaogm8dl6GlW"
      },
      "source": [
        "# pipeline_yb = Pipeline(pipeline_cluster.steps[:-1])\n",
        "# X_train_yb = pipeline_yb.transform(X_train)\n",
        "\n",
        "# from yellowbrick.model_selection import FeatureImportances\n",
        "# model = GradientBoostingClassifier(random_state=config.RANDOM_STATE)\n",
        "# viz = FeatureImportances(model)\n",
        "# viz.fit(X_train_yb, y_train)\n",
        "# viz.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RMHyCHm4ATv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQblpQISv0Fd"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "_parameters = {\n",
        "    'model__n_estimators':[50], # [100,200,50],\n",
        "    'model__max_depth': [3] # [None,3,10]\n",
        "}\n",
        "\n",
        "\n",
        "_pipe = GridSearchCV(\n",
        "\t\testimator = pipeline_clf_cluster,\n",
        "\t\tparam_grid = _parameters, \n",
        "\t\tcv=2,n_jobs=-2,verbose=2)\n",
        "_pipe.fit(X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4baRUyFv0IL"
      },
      "source": [
        "PipelineToDeploy = _pipe.best_estimator_\n",
        "PipelineToDeploy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZ0bHjDyrMr"
      },
      "source": [
        "_pipe.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwzpgGwys-x"
      },
      "source": [
        "X_train.columns[PipelineToDeploy['feat_selection'].get_support()].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUEqXK0yu1F"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print( classification_report(y_test, PipelineToDeploy.predict(X_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUo1Odq3kRD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "# Clusters Profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "* Main variables that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "source": [
        "df_cluster_profile = X.copy()\n",
        "for col in ['Cloud9am','Cloud3pm']:\n",
        "  df_cluster_profile[col] =df_cluster_profile[col].astype('object')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJD6gqqGkRU6"
      },
      "source": [
        "main_clusters_variables = ['MaxTemp', 'Humidity3pm', 'Cloud9am', 'Temp3pm']\n",
        "\n",
        "num_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(include=['number']).columns.to_list()\n",
        "categorical_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(exclude=['number']).columns.to_list()\n",
        "\n",
        "\n",
        "#['State','Cloud3pm','Pressure9am','RainToday']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOOWOX14vQw"
      },
      "source": [
        "## All Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bR8Q2kn4lx0"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "def PlotClustersDistribution(df,num_var,categorical_var):\n",
        "  for col in num_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(15,5));\n",
        "    sns.kdeplot(data=df, x=col, hue=\"Clusters\",palette='coolwarm')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "  for col in categorical_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(15,5));\n",
        "    sns.countplot(data=df.sort_values(by=col), hue=col, x=\"Clusters\",palette='coolwarm')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxpVsX_t9Pwz"
      },
      "source": [
        "PlotClustersDistribution(df=df_cluster_profile,num_var=num_var,categorical_var=categorical_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdkP_KIA4ylt"
      },
      "source": [
        "## Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLPADcb688lp"
      },
      "source": [
        "sns.set_style(\"darkgrid\")\n",
        "for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "  df_cluster = df_cluster_profile.query(f\"Clusters == {cluster}\")\n",
        "  print(f\"============ Cluster {cluster} ============\")\n",
        "  \n",
        "  for col in num_var:\n",
        "    print(f\"* {col} distribution for cluster {cluster}\")\n",
        "    plt.figure(figsize=(10,5));\n",
        "    sns.histplot(data=df_cluster, x=col)\n",
        "    plt.show();\n",
        "\n",
        "    iqr = df_cluster[col].quantile([0.25,0.75])\n",
        "    print(f\"* IQR: {iqr[0.25]} - {iqr[0.75]}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  for col in categorical_var:\n",
        "    print(f\"* {col} distribution for cluster {cluster}\")\n",
        "    try:\n",
        "      plt.figure(figsize=(10,5));\n",
        "      sns.countplot(data=df_cluster, x=col)\n",
        "      freq = df_cluster[col].value_counts()\n",
        "      plt.show()\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNE-CttzASt1"
      },
      "source": [
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umF5OfgrAy2d"
      },
      "source": [
        "iqr[0.25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFhp5afJA2ix"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65lkxPhlt8Uj"
      },
      "source": [
        "# df_cluster['Cloud3pm'].value_counts().sort_values().plot(kind='bar')\n",
        "\n",
        "df_cluster[col].dropna()#.value_counts().sort_values().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcaV2aJ0a01B"
      },
      "source": [
        "df_stack = X.filter([col,'Clusters'],axis=1).groupby([col,'Clusters']).size().reset_index()\n",
        "\n",
        "df_stack.rename({0:\"Count\"},axis=1,inplace=True)\n",
        "df_stack\n",
        "fig = px.bar(df_stack, x = 'Clusters',color=col, height=400)\n",
        "# fig.update_xaxes(type='category',categoryorder='category ascending')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqt5dieWdRGh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}