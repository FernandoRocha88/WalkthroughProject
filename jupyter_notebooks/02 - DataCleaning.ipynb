{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoRocha88/WalkthroughProject/blob/main/jupyter_notebooks/02%20-%20DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Cleaning Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Evaluate missing data\n",
        "*   Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/collection/WeatherAustralia.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate cleaned Train and Test sets, both saved under inputs/datasets/cleaned\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "  * Check: **Are all dates in the proper sequence (with no gaps) for each cities?**\n",
        "\n",
        "  * There are certain cities where some variables have 100% missing values\n",
        "\n",
        "* Missing Data Methods\n",
        "  * Drop Variables: \n",
        "  * Drop Rows: \n",
        "  * CategoricalImputer: \n",
        "  * Median Imputation: \n",
        "  * Mean Imputation:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGT0ZCtwFAFv"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcidnQspZztu"
      },
      "source": [
        "! pip install matplotlib -U\n",
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install missingno==0.4.2\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install ppscore==1.2.0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1anPl4iFEPR"
      },
      "source": [
        "# Code for restarting the runtime (that will restart colab session, all your variables will be lost)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit → Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "UserName = getpass('GitHub User Name: ')\n",
        "UserEmail = getpass('GitHub User E-mail: ')\n",
        "RepoName = getpass('GitHub Repository Name: ')\n",
        "UserPwd = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStVvDjfTxAk"
      },
      "source": [
        "! git clone https://github.com/{UserName}/{RepoName}.git\n",
        "! rm -rf sample_data\n",
        "\n",
        "print(\"\\n\")\n",
        "%cd /content/{RepoName}\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")\n",
        "print(f\"* You may refresh the session folder to access {RepoName} folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8MWs250vtR"
      },
      "source": [
        "!git config --global user.email {UserEmail}\n",
        "!git config --global user.name {UserName}\n",
        "!git remote rm origin\n",
        "!git remote add origin https://{UserName}:{UserPwd}@github.com/{UserName}/{RepoName}.git\n",
        "\n",
        "print(f\"\\n\\n * The current Colab Session is connected to the following GitHub repo: {UserName}/{RepoName}\")\n",
        "print(\" * You can now push new files to the repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwFQLlmwrl9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcmA1wG8AdC"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dafOBor8OoM"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkyUs70oloW"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXBDTg2ouLC"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_twMc7cefGw"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf {RepoName}\n",
        "print(f\"\\n * Please refresh session folder to validate that {RepoName} folder was removed from this session.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LEJkEZwl2K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Quick EDA with Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df=df, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c_xDPgMTrpw"
      },
      "source": [
        "# Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqBMq0WhYSt7"
      },
      "source": [
        "* which variables are more correlated with a given set of variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyh-atFTJUe"
      },
      "source": [
        "df_corr_spearman = df.corr(method=\"spearman\")\n",
        "df_corr_pearson = df.corr(method=\"pearson\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vph_vuF2XVnh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def heatmap_correlation(df_corr,CorrThreshold):\n",
        "  NumberOfColumns = len(df.columns)\n",
        "\n",
        "  if NumberOfColumns > 1:\n",
        "      mask = np.zeros_like(df_corr, dtype=np.bool)\n",
        "      mask[np.triu_indices_from(mask)] = True\n",
        "      mask[abs(df_corr) < CorrThreshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,8))\n",
        "      ax = sns.heatmap(data=df_corr,annot=True,\n",
        "                       xticklabels=True,yticklabels=True,mask=mask,\n",
        "                       cmap='viridis',annot_kws={\"size\": 8})\n",
        "      plt.ylim(NumberOfColumns,0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "def pairplot_correlation(df,transparency,hue=None):\n",
        "  \n",
        "  if hue == None:\n",
        "    fig = sns.pairplot(data=df,plot_kws={'alpha':transparency})\n",
        "  else:\n",
        "    fig = sns.pairplot(data=df,hue= hue,plot_kws={'alpha':transparency})\n",
        "  \n",
        "  for i, j in zip(*np.triu_indices_from(fig.axes, 1)):\n",
        "      fig.axes[i, j].set_visible(False)\n",
        "  \n",
        "  plt.figure(figsize=(20,8))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUXLq0Ezmsap"
      },
      "source": [
        "* **Correlation Analysis**\n",
        "  * Analyze how the target variable for your ML models are correlated with other variables (features and target)\n",
        "  * Analyze multi colinearity, that is, how the features are correlated among themselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTzO8xjdHk-"
      },
      "source": [
        "print(\"Correlation Heatmap - Spearman: evaluates monotonic relationship \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_spearman, CorrThreshold=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VykAYQPddHti"
      },
      "source": [
        "print(\"Correlation Heatmap - Pearson: evaluates the linear relationship between two continuous variables \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_pearson,CorrThreshold=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHid6tTJTsGr"
      },
      "source": [
        "# Power Predictive Score - PPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw9obEodJ3DZ"
      },
      "source": [
        "* Either load PPS analysis or calculate; then preprare for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUz0MjCVUSG"
      },
      "source": [
        "import ppscore as pps\n",
        "try:\n",
        "  pps_matrix_raw = pd.read_csv(\"/content/WalkthroughProject/outputs/data_cleaning/pps_analysis.csv\")\n",
        "except:\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix_raw.to_csv(\"/content/WalkthroughProject/outputs/data_cleaning/pps_analysis.csv\",index=False)\n",
        "\n",
        "pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BBWeV0Ke5s"
      },
      "source": [
        "* PPS score distribution\n",
        "* It helps to tell the PPS Threshold for relevant relationships. \n",
        "  * It is suggested that if Q3 (or 75%) is lower than 0.2, a pps greater than 0.2 is a relevant relationship\n",
        "  * If Q3 is greater than 0.2, pps values greater than Q3 are a relevant relationship "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyc5GqjmKMDm"
      },
      "source": [
        "pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfH4PfHSJ_Sx"
      },
      "source": [
        "* Function: Heatmap for PPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bMnEgQV9Tk"
      },
      "source": [
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "def heatmap_pps(df,PPS_Threshold):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < PPS_Threshold] = True\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(20,12))\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                         mask=mask,cmap='rocket_r', annot_kws={\"size\": 7})\n",
        "        \n",
        "        plt.ylim(len(df.columns),0)\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH4isTxFCiC9"
      },
      "source": [
        "print(f\"* PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "      f\"* The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "heatmap_pps(df=pps_matrix,PPS_Threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9zGtSs7WzfZ"
      },
      "source": [
        "* pps heatmap with target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FOfvanfW1SK"
      },
      "source": [
        "def heatmap_pps_target(df,NumberOfColumns):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "  fig, ax = plt.subplots(figsize=(20,8))\n",
        "  ax = sns.heatmap(\n",
        "          df,\n",
        "          xticklabels=True,\n",
        "          yticklabels=True,\n",
        "          annot=True,\n",
        "          cmap='coolwarm',\n",
        "          annot_kws={\"size\": 8})\n",
        "\n",
        "  plt.ylim(NumberOfColumns,0)\n",
        "  plt.show()\n",
        "\n",
        "heatmap_pps_target(df=pps_matrix_raw,NumberOfColumns=df.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYdZHyDhu4kG"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLj6ugpo7Dmu"
      },
      "source": [
        "## Convert data type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDRxwB3n7Hzv"
      },
      "source": [
        "convert_variables = ['Cloud9am','Cloud3pm']\n",
        "for var in convert_variables:\n",
        "  df[var] = df[var].astype('object')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFMfZo18cxUS"
      },
      "source": [
        "## Get Day, Month, Year, Weekday, IsWeekend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPSpZBCNc2fp"
      },
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['WeekDay']=df['Date'].dt.weekday\n",
        "df['IsWeekend'] = df['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "df.drop(axis=1,labels=['Date'],inplace=True)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoVRefhu2Bk"
      },
      "source": [
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcocWZkIx6nk"
      },
      "source": [
        "* Custom function to display missing data levels in a dataframe, it shows the aboslute levels, relative levels and data type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9CoLqBhO7ga"
      },
      "source": [
        "def EvaluateMissingData(df):\n",
        "  missing_data_absolute = df.isnull().sum()\n",
        "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "  df_missing_data = (pd.DataFrame(\n",
        "                          data= {\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                 \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                 \"DataType\":df.dtypes}\n",
        "                                  )\n",
        "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
        "                    .query(\"PercentageOfDataset > 0\")\n",
        "                    )\n",
        "\n",
        "  return df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrzaG-ZhEKt"
      },
      "source": [
        "* Check missing data levels for initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxvNnLAjxCSi"
      },
      "source": [
        "EvaluateMissingData(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsFb6_NYyFgB"
      },
      "source": [
        "* Missing data levels in a visual format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1U17dNovahA"
      },
      "source": [
        "import missingno as mi\n",
        "mi.matrix(df,figsize=(20,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsL8yYQEyOSt"
      },
      "source": [
        "## Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_52ePITy6pQ"
      },
      "source": [
        "\n",
        "* It is assumed you assessed already the missing data levels, did a quick EDA, checked correlation, so you are aware of the variables to work on\n",
        "\n",
        "---\n",
        "\n",
        "* **Strategy**\n",
        "* You will **iterate the steps below across different methods**, so at the end you will have dealt with all variables with missing data\n",
        "\n",
        "  * 1 -  Select a **method**\n",
        "  * 2 - Select **variables** to apply the method\n",
        "  * 3 - Create a **separate dataframe** applying this method to the selected variables\n",
        "  * 4 - **Compare** this new dataset with initial dataset to validate/assess the effect on distribution on variables\n",
        "  * 5 - **If** you are satisfied, **apply** the selected method to the initial dataframe\n",
        "  * 6 - **Evaluate** if you have more variables to deal. If yes, iterate. If not, you are done.\n",
        "\n",
        "---\n",
        "\n",
        "* Eventually, over the steps, you will need to assess a different aspect to evaluate which method you would consider next\n",
        "  * For example, you may be in a situation where you have 3 variables with high missing data levels. You may check the correlation among them to evaluate Multicollinearity. Then you will be in a better position to select the next method\n",
        "\n",
        "  ---\n",
        "\n",
        "* Over the course, you saw multiple forms for dealing with missing data, like DropVariables, DropNA, Imput with mean/median/mode, Imput the most frequent item etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8Yqjy6ghyw"
      },
      "source": [
        "### Data Cleaning Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GaYe_DgqwT"
      },
      "source": [
        "* List here the strategies you want initially to try. \n",
        "\n",
        "  * Drop Variables\n",
        "  * Drop Rows\n",
        "  * CategoricalImputer\n",
        "  * Median Imputation\n",
        "  * Mean Imputation\n",
        "\n",
        "* **The list above is your guide, your map to know in which stage you are in the data cleaning process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12Z8KIPoZ-8"
      },
      "source": [
        "### Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4T10lOcofen"
      },
      "source": [
        "* You have to split train and test set for cleaning the data\n",
        "* Unless you consider only Drop Variables and List wise deletion, which is not the case.\n",
        "* Hint: in the majority of the time in the workplace, you will need to split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knk7DcVborLI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from config import config\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['RainTomorrow'],\n",
        "                                        test_size=config.TEST_SIZE,\n",
        "                                        random_state=config.RANDOM_STATE)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdMPKBU_yCpF"
      },
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADd7Ruy2J8K"
      },
      "source": [
        "### DataCleaningEffect() function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrrjpCSCvLz8"
      },
      "source": [
        "* We create a custom function to evaluate variables distribution before and after applying the method. \n",
        "* It can be used accross the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_N7yZVnPBqR"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # select variables in which the given data cleaning method was not applied  \n",
        "  variables_not_applied_with_method = [x for x in df_cleaned.columns if x not in variables_applied_with_method]\n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if variable is numerical, plots histogram, if categorical, plots barplot\n",
        "  for set_of_variables in [variables_applied_with_method,variables_not_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:   \n",
        "        # it is categorical variable: barplot\n",
        "\n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"]).set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: \n",
        "        # it is numerical variable: histogram\n",
        "\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\")\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\").set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uATij1hg9-CH"
      },
      "source": [
        "### Template For Data Cleaning (Replace with method name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU1WfV1t9-CW"
      },
      "source": [
        "* Step 1: Method: **write here the method name and describe it**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUk1uIWA9-CX"
      },
      "source": [
        "##### list here the variables you want to apply the method\n",
        "variables_method = []\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkEkjOSO9-CX"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9Vq5BI9-CY"
      },
      "source": [
        "##### create a df_method dataframe applying your method to the TrainSet\n",
        "df_method = ....\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufMCbCC9-CY"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Oamd2q9-CY"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJsB1uh9-CZ"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSnOlmfd9-CZ"
      },
      "source": [
        "### Apply your method to the Train and Test Set\n",
        "TrainSet, TestSet = .....\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6iVZgMu9-CZ"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9id5pkns9-Ca"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUeGUUmu4qru"
      },
      "source": [
        "### Drop Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTFTio14qr8"
      },
      "source": [
        "* Hint: you may drop Variables with more than 80% of missing data, since these variables will likely not add much value. However, this is not the case in this dataset\n",
        "* Step 1: Method: **Drop Variables**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlQBeLo44qr8"
      },
      "source": [
        "variables_method = ['Sunshine', 'Evaporation','Cloud9am']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
        "    f\"{variables_method}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7au_UdA34qr9"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh9E2cEe4qr9"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsjRXMlU4qr9"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9pg4ww_BO-"
      },
      "source": [
        "* In this case, no effect on variables distribution, since you are not removing rows, but columns\n",
        "* The effect might be losing features that might have a relevant impact in your machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBq1a_Me4qr-"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGO5M1k44qr-"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LozxNCVO4qr_"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGmZy46L4qr_"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9E7EKFiAsuw"
      },
      "source": [
        "### Drop Rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSaadrA2Asu4"
      },
      "source": [
        "* Step 1: Method: **Drop Rows / Observations**\n",
        "* Hint: as a rule of thumb, you can drop the rows if a variable has less than 5% of missing data. However, you factor the effect of removing it, eventually for your dataset, 5% is a relevant amount of data or eventually these rows are significant for other variables.\n",
        "* You should also consider the variable distribution for that particular variables to decide wheter dropna() or not these rows.\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOujkgg3Asu5"
      },
      "source": [
        "variables_method = ['RainTomorrow', 'RainfallToday', 'RainToday','RainfallTomorrow']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDiuzFVAsu7"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXE-HzytAsu7"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "imputer = DropMissingData(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMqogFarAsu8"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXrGdmz3Asu8"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5_9TwaAsu8"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUvHvBlAsu9"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "imputer = DropMissingData(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6fVg28Asu9"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOZ6eWTuAsu-"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPmsfd5enMJS"
      },
      "source": [
        "### CategoricalImputer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZhlXhanMJY"
      },
      "source": [
        "* Step 1: Method: **The CategoricalImputer() replaces missing data in categorical variables with the string ‘Missing’**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbSBiHTBnMJY"
      },
      "source": [
        "variables_method = ['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm']\n",
        "print(f\"* {len(variables_method)} variables to apply the method \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3iiDrOnMJZ"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjowIPUnMJZ"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "imputer = CategoricalImputer(variables=variables_method,imputation_method='missing',fill_value='Missing')\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9R_Em4JnMJa"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFKQWlk0nMJa"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1YjY2CnMJa"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdpDbaKnMJb"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "imputer = CategoricalImputer(variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4PiB9wMnMJb"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3feT1lbnMJb"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9lrYiU4FH1"
      },
      "source": [
        "### Median Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uARU9Bjf4FIA"
      },
      "source": [
        "* Step 1: Method: **mediam imputation**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPUN4yLa4FIC"
      },
      "source": [
        "variables_method = ['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                    'WindSpeed9am','Temp9am','MaxTemp']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPuZGRkC4FID"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZJKY5Aj4FID"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYFuuEqh4FIE"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o3k-nMU4FIE"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quYOLkKX4FIF"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2OEFGEa4FIF"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrD9q87M4FIG"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d_DiCUG4FIG"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzlAb654LPY"
      },
      "source": [
        "### Mean Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0YSkpN4LPZ"
      },
      "source": [
        "* Step 1: Method: **mean imputation**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TffoUDqS4LPa"
      },
      "source": [
        "variables_method = ['MinTemp']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA21rw_04LPa"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeolVUEj4LPb"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_JVWwwl4LPb"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeXNWmG4LPc"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhoe836Z4LPc"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaCyiWO4LPd"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = imputer.transform(TrainSet), imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmYtf324LPe"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPD4bPIz4LPe"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjX8r9DN8eeh"
      },
      "source": [
        "EvaluateMissingData(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oexJ3aqG8rLS"
      },
      "source": [
        "* Well done! Your data is cleaned!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD--GI5Jvp8h"
      },
      "source": [
        "# Save cleaned data: Train/Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNWONrYwu6d"
      },
      "source": [
        "TrainSet.to_csv(\"/content/WalkthroughProject/outputs/datasets/cleaned/TrainSetCleaned.csv\",index=False)\n",
        "TestSet.to_csv(\"/content/WalkthroughProject/outputs/datasets/cleaned/TestSetCleaned.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMucnZHuD3CP"
      },
      "source": [
        "* You may now go to \"Push generated/new files from this session to GitHub Repo\" section and push these files to the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnS7v_HVb1db"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}