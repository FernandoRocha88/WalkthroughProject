{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoRocha88/WalkthroughProject/blob/main/jupyter_notebooks/02%20-%20DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Cleaning Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Evaluate missing data\n",
        "*   Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/collection/WeatherAustralia.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate cleaned Train and Test sets, both saved under inputs/datasets/cleaned\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "  * Check: **Are all dates in the proper sequence (with no gaps) for each cities?**\n",
        "\n",
        "  * There are certain cities where some variables have 100% missing values\n",
        "\n",
        "* Missing Data\n",
        "  * DropVariables: ['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']\n",
        "  * dropna() with less than 8%: ['WindDir9am', 'WindGustDir', 'WindGustSpeed', 'Humidity3pm', 'WindDir3pm', 'Temp3pm', 'RainTomorrow', 'RainToday', 'RainfallTomorrow', 'Rainfall', 'WindSpeed3pm', 'Humidity9am', 'Temp9am', 'WindSpeed9am', 'MinTemp', 'MaxTemp']\n",
        "  * imput with mediam: ['Pressure9am', 'Pressure3pm']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGT0ZCtwFAFv"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcidnQspZztu"
      },
      "source": [
        "! pip install matplotlib -U\n",
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install missingno==0.4.2\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install ppscore==1.2.0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1anPl4iFEPR"
      },
      "source": [
        "# Code for restarting the runtime (that will restart colab session, all your variables will be lost)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "UserName = getpass('GitHub User Name: ')\n",
        "UserEmail = getpass('GitHub User E-mail: ')\n",
        "RepoName = getpass('GitHub Repository Name: ')\n",
        "UserPwd = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStVvDjfTxAk"
      },
      "source": [
        "! git clone https://github.com/{UserName}/{RepoName}.git\n",
        "! rm -rf sample_data\n",
        "\n",
        "print(\"\\n\")\n",
        "%cd /content/{RepoName}\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")\n",
        "print(f\"* You may refresh the session folder to access {RepoName} folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8MWs250vtR"
      },
      "source": [
        "!git config --global user.email {UserEmail}\n",
        "!git config --global user.name {UserName}\n",
        "!git remote rm origin\n",
        "!git remote add origin https://{UserName}:{UserPwd}@github.com/{UserName}/{RepoName}.git\n",
        "\n",
        "print(f\"\\n\\n * The current Colab Session is connected to the following GitHub repo: {UserName}/{RepoName}\")\n",
        "print(\" * You can now push new files to the repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwFQLlmwrl9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcmA1wG8AdC"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dafOBor8OoM"
      },
      "source": [
        "CommitMsg = \"added=cleaned-data\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkyUs70oloW"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXBDTg2ouLC"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_twMc7cefGw"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf {RepoName}\n",
        "print(f\"\\n * Please refresh session folder to validate that {RepoName} folder was removed from this session.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LEJkEZwl2K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"/content/WalkthroughProject/inputs/datasets/collection/WeatherAustralia.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "## Quick exploration with Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "flag_minimal=True # True  False\n",
        "ProfileReport(df=df, minimal=flag_minimal).to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fYi8GapTGy4"
      },
      "source": [
        "## Correlation and PPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c_xDPgMTrpw"
      },
      "source": [
        "## Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqBMq0WhYSt7"
      },
      "source": [
        "* which variables are more correlated with a given set of variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyh-atFTJUe"
      },
      "source": [
        "df_corr_spearman = df.corr(method=\"spearman\")\n",
        "df_corr_pearson = df.corr(method=\"pearson\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vph_vuF2XVnh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def heatmap_correlation(df_corr,CorrThreshold):\n",
        "  NumberOfColumns = len(df.columns)\n",
        "\n",
        "  if NumberOfColumns > 1:\n",
        "      mask = np.zeros_like(df_corr, dtype=np.bool)\n",
        "      mask[np.triu_indices_from(mask)] = True\n",
        "      mask[abs(df_corr) < CorrThreshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,10))\n",
        "      ax = sns.heatmap(data=df_corr,annot=True,\n",
        "                       xticklabels=True,yticklabels=True,\n",
        "                       mask=mask,cmap='viridis',annot_kws={\"size\": 8})\n",
        "      plt.ylim(NumberOfColumns,0)\n",
        "      plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTzO8xjdHk-"
      },
      "source": [
        "print(\"Correlation Heatmap - Spearman: evaluates the monotonic relationship \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_spearman, CorrThreshold=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VykAYQPddHti"
      },
      "source": [
        "print(\"Correlation Heatmap - Pearson: evaluates the linear relationship between two continuous variables \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_pearson,CorrThreshold=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHid6tTJTsGr"
      },
      "source": [
        "## PPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUz0MjCVUSG"
      },
      "source": [
        "ppsMatrixRaw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpUgohXdTJts"
      },
      "source": [
        "import ppscore as pps\n",
        "# ppsMatrixRaw = pps.matrix(df.sample(frac=0.02))\n",
        "FullMatrix = (ppsMatrixRaw\n",
        "    [['x', 'y', 'ppscore']]\n",
        "    .pivot(columns='x', index='y', values='ppscore')\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bMnEgQV9Tk"
      },
      "source": [
        "def heatmap_pps(df,PPS_Threshold):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < PPS_Threshold] = True\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(20,8))\n",
        "        ax = sns.heatmap(\n",
        "            df, \n",
        "            annot=True,\n",
        "            xticklabels=True,\n",
        "            yticklabels=True,\n",
        "            mask=mask,\n",
        "            cmap='Blues',\n",
        "            annot_kws={\"size\": 7})\n",
        "        \n",
        "        plt.ylim(len(df.columns),0)\n",
        "        plt.show()\n",
        "\n",
        "heatmap_pps(df=FullMatrix,PPS_Threshold=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9zGtSs7WzfZ"
      },
      "source": [
        "* pps heatmap with target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FOfvanfW1SK"
      },
      "source": [
        "def heatmap_pps_target(df,NumberOfColumns):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "  fig, ax = plt.subplots(figsize=(20,8))\n",
        "  ax = sns.heatmap(\n",
        "          df,\n",
        "          xticklabels=True,\n",
        "          yticklabels=True,\n",
        "          annot=True,\n",
        "          cmap='coolwarm',\n",
        "          annot_kws={\"size\": 8})\n",
        "\n",
        "  plt.ylim(NumberOfColumns,0)\n",
        "  plt.show()\n",
        "\n",
        "heatmap_pps_target(df=FullMatrix,NumberOfColumns=df.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYdZHyDhu4kG"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFMfZo18cxUS"
      },
      "source": [
        "## Get Day, Month and Year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPSpZBCNc2fp"
      },
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "# week day\n",
        "# is weekend\n",
        "df.drop(axis=1,labels=['Date'],inplace=True)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIeDzZ33c263"
      },
      "source": [
        "## Add RainfallTomorrow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv5ezghRc-uk"
      },
      "source": [
        "def AddRainfallTomorrow(df,categ_var='Location'):\n",
        "  df_final = pd.DataFrame([])\n",
        "\n",
        "  for city in df[categ_var].unique():\n",
        "    dfCity = df.query(f\"{categ_var} == '{city}'\").copy()\n",
        "    dfCity['RainfallTomorrow'] = df['Rainfall'].shift(-1)\n",
        "    df_final = df_final.append(dfCity)\n",
        "\n",
        "  return df_final\n",
        "\n",
        "df = AddRainfallTomorrow(df)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJxogPigdqG"
      },
      "source": [
        "* Are all dates in the proper sequence (with no gaps) for each cities?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRDE-p1AglGC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoVRefhu2Bk"
      },
      "source": [
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcocWZkIx6nk"
      },
      "source": [
        "* Custom function to display missing data levels in a dataframe, it shows the aboslute levels, relative levels and data type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9CoLqBhO7ga"
      },
      "source": [
        "def EvaluateMissingData(df):\n",
        "  missing_data_absolute = df.isnull().sum()\n",
        "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "  df_missing_data = (pd.DataFrame(\n",
        "                          data= {\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                 \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                 \"DataType\":df.dtypes}\n",
        "                                  )\n",
        "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
        "                    .query(\"PercentageOfDataset > 0\")\n",
        "                    )\n",
        "\n",
        "  return df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrzaG-ZhEKt"
      },
      "source": [
        "* Check missing data levels for initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxvNnLAjxCSi"
      },
      "source": [
        "EvaluateMissingData(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsFb6_NYyFgB"
      },
      "source": [
        "* Missing data levels in a visual format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1U17dNovahA"
      },
      "source": [
        "import missingno as mi\n",
        "mi.matrix(df,figsize=(20,6))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEpmPY9GHjGL"
      },
      "source": [
        "### What are the rows with missing data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heSgg7BTHmK0"
      },
      "source": [
        "df_rows_with_NA = df[df.isnull().any(axis=1)].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx8mUrDtA4co"
      },
      "source": [
        "* which cities have more missing data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSFvY5ZcA2gP"
      },
      "source": [
        "df_cities_with_NA = pd.DataFrame(data={\n",
        "                                  \"MissingDataRows\":df_rows_with_NA['Location'].sort_values().value_counts(),\n",
        "                                  \"TotalNumberOfRows\":df['Location'].sort_values().value_counts()\n",
        "                                  })\n",
        "\n",
        "df_cities_with_NA.plot(kind='bar',figsize=(20,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo-M-zjxD0Ta"
      },
      "source": [
        "* There are some cities where some variables have 100% missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iruiMFcELCh"
      },
      "source": [
        "df_cities_with_NA['Difference'] = df_cities_with_NA['TotalNumberOfRows'] - df_cities_with_NA['MissingDataRows']\n",
        "\n",
        "list_of_cities_with_NA = df_cities_with_NA.query(f\"Difference == 0\").index.to_list()\n",
        "print(f\"* There are {len(list_of_cities_with_NA)} cities where some variables have 100% of missing values. \\n\"\n",
        "      f\"* These are the cities: {list_of_cities_with_NA}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20pftRH0GLHd"
      },
      "source": [
        "for city in list_of_cities_with_NA:\n",
        "  df_aux = df.query(f\"Location == '{city}'\")\n",
        "  list_of_variables_with_100perc_missing_data = df_aux.columns[df_aux.isna().sum() / len(df_aux) == 1].to_list()\n",
        "\n",
        "  print(f\"* {city} \\n \"\n",
        "        f\"Variables with 100% missing data: {list_of_variables_with_100perc_missing_data} \\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsL8yYQEyOSt"
      },
      "source": [
        "## Dealing with Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_52ePITy6pQ"
      },
      "source": [
        "\n",
        "* It is assumed you assessed already the missing data levels, did a quick EDA, checked correlation, so you are aware of the variables to work on\n",
        "\n",
        "---\n",
        "\n",
        "* **Strategy**\n",
        "* You will **iterate the steps below accross different methods**, so at the end you will have dealt with all variables with missing data\n",
        "\n",
        "  * 1 -  Select a **method**\n",
        "  * 2 - Select **variables** to apply the method\n",
        "  * 3 - Create a **separate dataframe** applying this method to the selected variables\n",
        "  * 4 - **Compare** this new dataset with initial dataset to validate/assess the effect on distribution on variables\n",
        "  * 5 - **If** you are satisfied, **apply** the selected method to the initial dataframe\n",
        "  * 6 - **Evaluate** if you have more variables to deal. If yes, iterate. If not, you are done.\n",
        "\n",
        "---\n",
        "\n",
        "* Eventually, over the steps, you will need to assess a different aspect to evaluate which method you would consider next\n",
        "  * For example, you may be in a situation where you have 3 variables with high missing data levels. You may check the correlation among them to evaluate Multicollinearity. Then you will be in a better position to select the next method\n",
        "\n",
        "  ---\n",
        "\n",
        "* Over the course, you saw multiple methods for dealing iwth missing data, like DropVariables, DropNA, Imput with mean/median/mode, Imput the most frequent item etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8Yqjy6ghyw"
      },
      "source": [
        "### Data Cleaning Methods Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GaYe_DgqwT"
      },
      "source": [
        "* List here the methods you want initially to try. \n",
        "\n",
        "  * Drop Variables\n",
        "  * List wise deletion\n",
        "  * CategoricalImputer\n",
        "  * Reduce cardinaliy for categorical (feat engineering)\n",
        "  * Median Imputation\n",
        "  * Mean Imputation\n",
        "\n",
        "* **The list above is your guide, your map to know in which stage you are in the data cleaning process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12Z8KIPoZ-8"
      },
      "source": [
        "### Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4T10lOcofen"
      },
      "source": [
        "* You have to split train and test set for cleaning the data\n",
        "* Unless you consider only Drop Variables and List wise deletion, which is not the case.\n",
        "* Hint: in the majority of the time in the workplace, you will need to split into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knk7DcVborLI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from config import config\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['RainTomorrow'],\n",
        "                                        test_size=config.TEST_SIZE,\n",
        "                                        random_state=config.RANDOM_STATE)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdMPKBU_yCpF"
      },
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vADd7Ruy2J8K"
      },
      "source": [
        "### DataCleaningEffect() function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrrjpCSCvLz8"
      },
      "source": [
        "* We create a custom function to evaluate variables distribution before and after applying the method. \n",
        "* It can be used accross the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_N7yZVnPBqR"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # select variables in which the given data cleaning method was not applied  \n",
        "  variables_not_applied_with_method = [x for x in df_cleaned.columns if x not in variables_applied_with_method]\n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if variable is numerical, plots histogram, if categorical, plots barplot\n",
        "  for set_of_variables in [variables_applied_with_method,variables_not_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:   \n",
        "        # it is categorical variable: barplot\n",
        "\n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"]).set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: \n",
        "        # it is numerical variable: histogram\n",
        "\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\")\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\").set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUeGUUmu4qru"
      },
      "source": [
        "### Template For Data Cleaning (Replace with method name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTFTio14qr8"
      },
      "source": [
        "* Step 1: Method: **write here the method name and describe it**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlQBeLo44qr8"
      },
      "source": [
        "##### list here the variables you want to apply the method\n",
        "variables_method = []\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7au_UdA34qr9"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh9E2cEe4qr9"
      },
      "source": [
        "##### create a df_method dataframe using the TrainSet\n",
        "df_method = ....\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsjRXMlU4qr9"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBQIC-dp4qr-"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBq1a_Me4qr-"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGO5M1k44qr-"
      },
      "source": [
        "### You have to apply to the Train and Test Set your method\n",
        "TrainSet, TestSet = .....\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LozxNCVO4qr_"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGmZy46L4qr_"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s11GUReTPlo"
      },
      "source": [
        "### Drop variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS19aDUsgHcO"
      },
      "source": [
        "* Hint: you may **drop Variables with more than 80% of missing data**, since these variables will likely not add much value. However, this is not the case in this dataset\n",
        "* Method: Drop Variables\n",
        "* Select variables to apply the method\n",
        "* **just to speed up the process, we will drop variables with more than 30%, I will come back later and consider other methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-R0YuhNTP-R"
      },
      "source": [
        "# variables_drop = EvaluateMissingData(df).query(f\"PercentageOfDataset > 30\").index.to_list()\n",
        "\n",
        "# print(f\"* {len(variables_drop)} variables to drop \\n\\n\"\n",
        "#     f\"{variables_drop}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDsA7w2_iPWq"
      },
      "source": [
        "variables_drop = ['Sunshine', 'Evaporation','Cloud3pm']\n",
        "\n",
        "print(f\"* {len(variables_drop)} variables to drop \\n\\n\"\n",
        "    f\"{variables_drop}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ0vCtUUkhh0"
      },
      "source": [
        "* Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGzxfynve1WV"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "drop_imputer = DropFeatures(features_to_drop=variables_drop)\n",
        "drop_imputer.fit(TrainSet)\n",
        "\n",
        "df_drop_columns =  drop_imputer.transform(TrainSet)\n",
        "\n",
        "lost_percentage = round(100- len(df_drop_columns) / len(TrainSet) *100,2) \n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of all dataset, or {len(TrainSet)-len(df_drop_columns)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(TrainSet)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_drop_columns)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_R6j5sCk2Ng"
      },
      "source": [
        "* What is the effect?\n",
        "    * In this case, no effect on variables distribution, since you are not removing rows, but columns\n",
        "     * The effect is losing features that might have a relevant impact in your machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7w7JxhzlWay"
      },
      "source": [
        "* If you are statisfied, apply the method to the Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loKjRa4hqKZb"
      },
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "drop_imputer = DropFeatures(features_to_drop=variables_drop)\n",
        "\n",
        "drop_imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = drop_imputer.transform(TrainSet) , drop_imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_r2d4vCNJou"
      },
      "source": [
        "* Evaluate if you have more variables to deal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWgcfYN4sQeY"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q74x5KIrRW7T"
      },
      "source": [
        "### Complete Case Analysis (\"list-wise deletion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYtnc0u4X_pS"
      },
      "source": [
        "* Hint: as a rule of thumb, you may **remove missing observations if a  variable has less than 5% of missing data**. However, you should assess the effect of removing, eventually for your dataset, 5% is a lot of data.\n",
        "* You should also consider the variable distribution to decide wheter dropna() or not\n",
        "* Select variables to apply the method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2fmXi8LXycz"
      },
      "source": [
        "# variables_cca = (EvaluateMissingData(df)\n",
        "#                 .query(\"PercentageOfDataset < 8\")\n",
        "#                 .index\n",
        "#                 .to_list()\n",
        "#                 )\n",
        "# print(f\"* {len(variables_cca)} variables to apply Complete Case Analysis \\n\\n\"\n",
        "#     f\"{variables_cca}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lQlDSLwjZJf"
      },
      "source": [
        "variables_cca = ['RainTomorrow', 'Rainfall', 'RainToday','RainfallTomorrow','MaxTemp']\n",
        "\n",
        "print(f\"* {len(variables_cca)} variables to apply Complete Case Analysis \\n\\n\"\n",
        "    f\"{variables_cca}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-m46ryMh_hZ"
      },
      "source": [
        "* Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaJzeULTXzR5"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "imputer = DropMissingData(variables=variables_cca)\n",
        "imputer.fit(TrainSet)\n",
        "df_cca = imputer.transform(TrainSet)\n",
        "\n",
        "lost_percentage = round(100- len(df_cca) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_cca)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_cca)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x5UklFadqys"
      },
      "source": [
        "* What is the effect?  \n",
        "  * We can plot the distribution before and after applying the method. If the distribution differs a lot, better not consider this method for that variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qRdvx-7I4DK"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_cca,\n",
        "                  variables_applied_with_method = variables_cca\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6V7A-gflM56"
      },
      "source": [
        "* If you are statisfied, apply the method in your TrainSet and TestSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfUd2IqJRcu6"
      },
      "source": [
        "from feature_engine.imputation import DropMissingData\n",
        "missingdata_imputer = DropMissingData(variables=variables_cca)\n",
        "missingdata_imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = missingdata_imputer.transform(TrainSet) , missingdata_imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYC8xUoHNklD"
      },
      "source": [
        "* Evaluate if you have more variables to deal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJLfflIRTABo"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPmsfd5enMJS"
      },
      "source": [
        "### CategoricalImputer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZhlXhanMJY"
      },
      "source": [
        "* Step 1: Method: The CategoricalImputer() replaces missing data in categorical variables with the string â€˜Missingâ€™\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbSBiHTBnMJY"
      },
      "source": [
        "variables_catimp = ['WindDir9am', 'WindGustDir', 'WindDir3pm']\n",
        "\n",
        "# 'Cloud9am'\n",
        "\n",
        "print(f\"* {len(variables_catimp)} variables to apply Categorical Imputer \\n\\n\"\n",
        "    f\"{variables_catimp}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3iiDrOnMJZ"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InjowIPUnMJZ"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "imputer = CategoricalImputer(variables=variables_catimp)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)\n",
        "\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9R_Em4JnMJa"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFKQWlk0nMJa"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_catimp\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1YjY2CnMJa"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgdpDbaKnMJb"
      },
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "categ_imputer = CategoricalImputer(variables=variables_catimp)\n",
        "categ_imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = categ_imputer.transform(TrainSet) , categ_imputer.transform(TestSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4PiB9wMnMJb"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3feT1lbnMJb"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW9lrYiU4FH1"
      },
      "source": [
        "### Median Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uARU9Bjf4FIA"
      },
      "source": [
        "* Step 1: Method: **mediam imputation**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Yz95sL4TOn"
      },
      "source": [
        "EvaluateMissingData(TrainSet).index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPUN4yLa4FIC"
      },
      "source": [
        "variables_method = ['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                    'WindSpeed9am','Temp9am']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPuZGRkC4FID"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZJKY5Aj4FID"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)\n",
        "\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYFuuEqh4FIE"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o3k-nMU4FIE"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quYOLkKX4FIF"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2OEFGEa4FIF"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "median_imputer = MeanMedianImputer(imputation_method='median',variables=variables_method)\n",
        "median_imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = median_imputer.transform(TrainSet), median_imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrD9q87M4FIG"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d_DiCUG4FIG"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdzlAb654LPY"
      },
      "source": [
        "### Mean Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0YSkpN4LPZ"
      },
      "source": [
        "* Step 1: Method: **write here the method name and describe it**\n",
        "* Step 2: Select variables to apply the method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TffoUDqS4LPa"
      },
      "source": [
        "##### list here the variables you want to apply the method\n",
        "variables_method = ['MinTemp']\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to apply this method. \\n\\n\"\n",
        "    f\"{variables_method}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA21rw_04LPa"
      },
      "source": [
        "* Step 3: Create a separate dataframe applying this method to the selected variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeolVUEj4LPb"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method= imputer.transform(TrainSet)\n",
        "\n",
        "\n",
        "lost_percentage = round(100- len(df_method) / len(TrainSet) *100,2)\n",
        "\n",
        "print(f\"* If I apply this method, \"\n",
        "      f\"I will lose {lost_percentage}% of the dataset, or {len(TrainSet)-len(df_method)} rows. \\n\"\n",
        "      f\"* Dataset rows before method: {len(df)} \\n\"\n",
        "      f\"* Dataset rows after method: {len(df_method)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_JVWwwl4LPb"
      },
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeXNWmG4LPc"
      },
      "source": [
        "DataCleaningEffect(\n",
        "                  df_original = TrainSet,\n",
        "                  df_cleaned = df_method,\n",
        "                  variables_applied_with_method = variables_method\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhoe836Z4LPc"
      },
      "source": [
        "* Step 5: If you are statisfied, apply the method in your dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaCyiWO4LPd"
      },
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "median_imputer = MeanMedianImputer(imputation_method='mean',variables=variables_method)\n",
        "median_imputer.fit(TrainSet)\n",
        "TrainSet, TestSet = median_imputer.transform(TrainSet), median_imputer.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmYtf324LPe"
      },
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPD4bPIz4LPe"
      },
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD--GI5Jvp8h"
      },
      "source": [
        "# Save cleaned data: Train/Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNWONrYwu6d"
      },
      "source": [
        "TrainSet.to_csv(\"/content/WalkthroughProject1/inputs/datasets/cleaned/TrainSetCleaned.csv\",index=False)\n",
        "TestSet.to_csv(\"/content/WalkthroughProject1/inputs/datasets/cleaned/TestSetCleaned.csv\",index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frOXm8Fz8QM4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Your next notebook section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0oBmdCWb0xf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnS7v_HVb1db"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}