{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Regression Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a regression model to predict tomorrow's rainfall levels, in mm.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Regression model\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "! pip install yellowbrick==1.2\n",
        "! pip install lazypredict==0.2.9\n",
        "\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz",
        "outputId": "a8baab65-8510-4021-85f5-43cd5c5bceaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Thanks for inserting your credentials!\n",
            "* You may now Clone your Repo to this Session, then Connect this Session to your Repo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1",
        "outputId": "e94e7367-38a1-4695-a003-100677122d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'WalkthroughProject' already exists and is not an empty directory.\n",
            "\n",
            "\n",
            "/content/WalkthroughProject\n",
            "\n",
            "\n",
            "* Current session directory is:/content/WalkthroughProject\n",
            "* You may refresh the session folder to access WalkthroughProject folder.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "# import uuid\n",
        "# file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "# with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "# print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "# print(\"\\n\\n\")\n",
        "# os.remove(f\"{file_name}.txt\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8",
        "outputId": "211df5fc-0680-4bcf-ddbb-4bcd7f30a93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\")\n",
        "      .query(\"RainTomorrow == 'Yes'\")  # subset RainTomorrow as Yes\n",
        "      .drop(labels=['RainTomorrow'],axis=1)\n",
        "      .dropna(subset=['RainfallTomorrow'])   # drop missing data from target RainfallTomorrow\n",
        "  )\n",
        "\n",
        "\n",
        "# subset RainTomorrow as 1, label: RainfallTomorrow, features: all other variables\n",
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 31838 entries, 8 to 145393\n",
            "Data columns (total 26 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Date              31838 non-null  object \n",
            " 1   Location          31838 non-null  object \n",
            " 2   MinTemp           31663 non-null  float64\n",
            " 3   MaxTemp           31783 non-null  float64\n",
            " 4   RainfallToday     31162 non-null  float64\n",
            " 5   Evaporation       17836 non-null  float64\n",
            " 6   Sunshine          16801 non-null  float64\n",
            " 7   WindGustDir       29374 non-null  object \n",
            " 8   WindGustSpeed     29399 non-null  float64\n",
            " 9   WindDir9am        29921 non-null  object \n",
            " 10  WindDir3pm        30786 non-null  object \n",
            " 11  WindSpeed9am      31498 non-null  float64\n",
            " 12  WindSpeed3pm      31155 non-null  float64\n",
            " 13  Humidity9am       31304 non-null  float64\n",
            " 14  Humidity3pm       30874 non-null  float64\n",
            " 15  Pressure9am       28740 non-null  float64\n",
            " 16  Pressure3pm       28730 non-null  float64\n",
            " 17  Cloud9am          20600 non-null  float64\n",
            " 18  Cloud3pm          20266 non-null  float64\n",
            " 19  Temp9am           31540 non-null  float64\n",
            " 20  Temp3pm           31096 non-null  float64\n",
            " 21  RainToday         31162 non-null  object \n",
            " 22  RainfallTomorrow  31838 non-null  float64\n",
            " 23  Latitude          31838 non-null  float64\n",
            " 24  Longitude         31838 non-null  float64\n",
            " 25  State             31838 non-null  object \n",
            "dtypes: float64(19), object(7)\n",
            "memory usage: 6.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Regressor Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kD7PZ5kZkBT"
      },
      "source": [
        "## Custom transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96j6zLKZz7N"
      },
      "source": [
        "  * convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "  * get Get Day, Month, Year, Weekday, IsWeekend from Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTrXFaWSIU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "class ConvertToCategorical(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variables=None):\n",
        "      if not isinstance(variables, list):\n",
        "          self.variables = [variables]\n",
        "      else:\n",
        "          self.variables = variables\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      for feature in self.variables:\n",
        "          X[feature] = X[feature].astype('object')\n",
        "\n",
        "      return X\n",
        "\n",
        "\n",
        "# Get Day, Month, Year, Weekday, IsWeekend from Date\n",
        "class GetFeaturesFromDate(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variable=None):\n",
        "      self.variable = variable\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      X[self.variable] = pd.to_datetime(X[self.variable])\n",
        "      X['Day'] = X[self.variable].dt.day\n",
        "      X['Month'] = X[self.variable].dt.month\n",
        "      X['Year'] = X[self.variable].dt.year\n",
        "      X['WeekDay']= X[self.variable].dt.weekday\n",
        "      X['IsWeekend'] = X['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "      return X\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML Pipeline: DataCleaningFeatEng, and Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA",
        "outputId": "d8103681-dd1e-4f74-f3a9-5ebf7802a0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.imputation import AddMissingIndicator\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import DropMissingData\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      ### Data Cleaning\n",
        "      (\"ConvertToCategorical\",ConvertToCategorical(variables = ['Cloud9am','Cloud3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"GetFeaturesFromDate\",GetFeaturesFromDate(variable= 'Date')\n",
        "      ),\n",
        "       \n",
        "      (\"AddMissingIndicator\",AddMissingIndicator(variables= ['Sunshine', 'Evaporation', 'Cloud3pm',\n",
        "                                                             'Cloud9am', 'Pressure9am', 'Pressure3pm',\n",
        "                                                             'WindDir9am', 'WindGustDir', 'WindGustSpeed',\n",
        "                                                             'Humidity3pm', 'WindDir3pm', 'Temp3pm',\n",
        "                                                             'RainfallToday', 'RainToday',\n",
        "                                                             'WindSpeed3pm', 'Humidity9am','Temp9am',\n",
        "                                                             'WindSpeed9am', 'MinTemp','MaxTemp'])\n",
        "      ),\n",
        "\n",
        "      (\"DropFeatures\",DropFeatures(features_to_drop = ['Sunshine','Evaporation','Cloud9am','Date'])\n",
        "      ),                                         ##########dont drop sunshine\n",
        "\n",
        "      (\"DropMissingData\",DropMissingData(variables =['RainfallToday', 'RainToday'])\n",
        "      ),\n",
        "\n",
        "      (\"CategoricalImputer\",CategoricalImputer(variables=['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm'],\n",
        "                                                imputation_method='missing',fill_value='Missing')\n",
        "      ),\n",
        "\n",
        "      (\"MedianImputer\",MeanMedianImputer(imputation_method='median',\n",
        "                                          variables=['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                                                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                                                    'WindSpeed9am','Temp9am','MaxTemp',\n",
        "                                                     'RainfallToday']\n",
        "                                          )\n",
        "      ),\n",
        "\n",
        "      (\"MeanImputer\",MeanMedianImputer(imputation_method='mean',variables=['MinTemp'])\n",
        "      ),\n",
        "\n",
        "      ### Feature Engineering\n",
        "\n",
        "      (\"Winsorizer_iqr\",Winsorizer(capping_method='iqr',tail='both', fold=3,variables = ['RainfallToday'])\n",
        "      ),\n",
        "\n",
        "\n",
        "      (\"PowerTransformer\",PowerTransformer(variables = ['WindSpeed3pm','Humidity3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"YeoJohnsonTransformer\",YeoJohnsonTransformer(variables=['RainfallToday','WindGustSpeed',\n",
        "                                                                'WindSpeed9am','Humidity9am'])\n",
        "      ),\n",
        "\n",
        "      (\"EqualFrequencyDiscretiser\",EqualFrequencyDiscretiser(q=5,variables = ['Latitude','Longitude' ])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol5\",RareLabelEncoder(tol=0.05, n_categories=2, variables=['WindDir3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol7\",RareLabelEncoder(tol=0.06, n_categories=2, variables=['State'])\n",
        "      ),\n",
        "\n",
        "      (\"CountEncoder\",CountFrequencyEncoder(encoding_method='count',\n",
        "                                            variables = ['Location','WindGustDir','WindDir9am',\n",
        "                                                          'WindDir3pm','State','Cloud3pm',\n",
        "                                                          'RainToday'])\n",
        "      )\n",
        "\n",
        "    ]\n",
        "  )\n",
        "  return pipeline_base\n",
        "\n",
        "\n",
        "def PipelineRegressor():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        " \n",
        "  pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"model\",DecisionTreeRegressor(random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  return pipe\n",
        "\n",
        "\n",
        "\n",
        "PipelineRegressor()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('ConvertToCategorical',\n",
              "                 ConvertToCategorical(variables=['Cloud9am', 'Cloud3pm'])),\n",
              "                ('GetFeaturesFromDate', GetFeaturesFromDate(variable='Date')),\n",
              "                ('AddMissingIndicator',\n",
              "                 AddMissingIndicator(variables=['Sunshine', 'Evaporation',\n",
              "                                                'Cloud3pm', 'Cloud9am',\n",
              "                                                'Pressure9am', 'Pressure3pm',\n",
              "                                                'WindDir9am', 'WindGustDir',\n",
              "                                                'WindGustSpeed', 'Humidity3pm',\n",
              "                                                'WindD...\n",
              "                 RareLabelEncoder(n_categories=2, variables=['WindDir3pm'])),\n",
              "                ('RareLabelEncoder_tol7',\n",
              "                 RareLabelEncoder(n_categories=2, tol=0.06,\n",
              "                                  variables=['State'])),\n",
              "                ('CountEncoder',\n",
              "                 CountFrequencyEncoder(variables=['Location', 'WindGustDir',\n",
              "                                                  'WindDir9am', 'WindDir3pm',\n",
              "                                                  'State', 'Cloud3pm',\n",
              "                                                  'RainToday'])),\n",
              "                ['scaler', StandardScaler()],\n",
              "                ['model', DecisionTreeRegressor(random_state=0)]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMbLiVQkD3Y"
      },
      "source": [
        "# Lazy Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIiC8A3dkPQA"
      },
      "source": [
        "* Transform the data using pipeline, except last step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjKHFaC6kPYW",
        "outputId": "ad10a4f0-3030-4150-ae18-8472df6780ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline_lazy = Pipeline(PipelineRegressor().steps[:-2])\n",
        "columns_after_data_cleaning_feat_eng = pipeline_lazy.fit_transform(df).columns\n",
        "columns_after_data_cleaning_feat_eng\n",
        "\n",
        "pipeline_lazy = Pipeline(PipelineRegressor().steps[:-1])\n",
        "df_lazy = pipeline_lazy.fit_transform(df)\n",
        "df_lazy = pd.DataFrame(data = df_lazy,\n",
        "                       columns = columns_after_data_cleaning_feat_eng)\n",
        "\n",
        "df_lazy.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31162, 47)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUWVwx6akLzR"
      },
      "source": [
        "* Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiRtbU31kL84",
        "outputId": "8bd6cebf-8770-4b56-f5db-e90d8c6ed989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_lazy.drop(['RainfallTomorrow'],axis=1),\n",
        "                                    df_lazy['RainfallTomorrow'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24929, 46) (6233, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk0thDMvnN2g",
        "outputId": "79786aa6-fc32-4a57-e2cf-2ae297fc49a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape,y_test.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24929, 46), (24929,), (6233, 46), (6233,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBmumDTakHco"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIr5aPEokHiE",
        "outputId": "15264bca-398a-476f-cd0f-81d1e2675f07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "reg = LazyRegressor(ignore_warnings=False, predictions=True, random_state=config.RANDOM_STATE)\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c6a0db21773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlazypredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazyRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4uBw1mxnJIO",
        "outputId": "57a40958-509f-42d8-c9c6-88cb37e36d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-181ea416d56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "* Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD6B3CuhiDMT"
      },
      "source": [
        "* Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFzP2iGiIk1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['RainfallTomorrow'],axis=1),\n",
        "                                    df['RainfallTomorrow'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt60YblgiF0y"
      },
      "source": [
        "* Use lazy-predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLkUcA7riJD0"
      },
      "source": [
        "* Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imwlDdi6iJK-"
      },
      "source": [
        "pipeline_regressor = PipelineRegressor()\n",
        "pipeline_regressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "X = df.copy()\n",
        "\n",
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU1E4IdkyLd"
      },
      "source": [
        "* Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lx5EqalOtv"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk0uyz1lSIW"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJzUxAmkv07"
      },
      "source": [
        "* The goal is to merge cluster labels to our data.\n",
        "  * However, the pipeline dropped rows from ['RainfallToday', 'RainToday'] and had AddMissingIndicatorFlag in the process\n",
        "  * Before merging, we need to adjust it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUY6-_6A-rN"
      },
      "source": [
        "drop_imputer = DropMissingData(variables =['RainfallToday' , 'RainToday'])  #,'RainfallTomorrow','RainTomorrow'])\n",
        "X = drop_imputer.fit_transform(X)\n",
        "\n",
        "na_imputer =  AddMissingIndicator(variables= ['Sunshine', 'Evaporation', 'Cloud3pm',\n",
        "                                           'Cloud9am', 'Pressure9am', 'Pressure3pm',\n",
        "                                            'WindDir9am', 'WindGustDir', 'WindGustSpeed',\n",
        "                                            'Humidity3pm', 'WindDir3pm', 'Temp3pm',\n",
        "                                            #  'RainfallTomorrow','RainTomorrow',  ##########\n",
        "                                            'RainfallToday', 'RainToday',\n",
        "                                            'WindSpeed3pm', 'Humidity9am','Temp9am',\n",
        "                                            'WindSpeed9am', 'MinTemp','MaxTemp'])\n",
        "X = na_imputer.fit_transform(X)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "* We add a column \"Cluster\" to the data and check clusters distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "X['Clusters'] = X['Clusters'].astype('object')\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OhXSh7536Ye"
      },
      "source": [
        "* Clusters don't look to be imbalanced\n",
        "* This is how our data look like from now\n",
        "  * Check the last column: Clusters\n",
        "  * Quick reminder: The data is unprocessed (data cleaning, feat eng); except for the part DropMissingData(variables =['RainfallToday', 'RainToday'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "source": [
        "print(X.shape)\n",
        "X.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Regressor Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJPFLoTciOI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "# Classifier to explain cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "* We need to find the most relevant variables, to define each cluster in terms of each relevant variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "source": [
        "df_clf = X.copy() #.sample(frac=0.051, random_state=config.RANDOM_STATE)\n",
        "df_clf['Clusters'] = df_clf['Clusters'].astype('int32')\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE,\n",
        "                                    stratify=df_clf['Clusters']\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "* Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9nz9-UeBl0P"
      },
      "source": [
        "pipeline_clf_cluster['MedianImputer'].imputer_dict_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Cjsn6l5cqF"
      },
      "source": [
        "* Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QPwTqa0OcY"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train,y_train)\n",
        "\n",
        "# do GridCV after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        " * Evaluate model performance on Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\n",
        "      classification_report(y_train, pipeline_clf_cluster.predict(X_train))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "source": [
        "print(\n",
        "      classification_report(y_test, pipeline_clf_cluster.predict(X_test))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "* Check main features importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N7Mc3mrAYLq"
      },
      "source": [
        "df_feature_importance = pd.DataFrame(data={\n",
        "    'Attribute': df_clf.columns[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_\n",
        "  })\n",
        "\n",
        "df_feature_importance.sort_values(by='Importance', ascending=False).plot(kind='bar',x='Attribute',y='Importance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQblpQISv0Fd"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "_parameters = {\n",
        "    'model__n_estimators':[50], # [100,200,50],\n",
        "    'model__max_depth': [3] # [None,3,10]\n",
        "}\n",
        "\n",
        "\n",
        "_pipe = GridSearchCV(\n",
        "\t\testimator = pipeline_clf_cluster,\n",
        "\t\tparam_grid = _parameters, \n",
        "\t\tcv=2,n_jobs=-2,verbose=2)\n",
        "_pipe.fit(X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4baRUyFv0IL"
      },
      "source": [
        "PipelineToDeploy = _pipe.best_estimator_\n",
        "PipelineToDeploy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZ0bHjDyrMr"
      },
      "source": [
        "_pipe.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwzpgGwys-x"
      },
      "source": [
        "X_train.columns[PipelineToDeploy['feat_selection'].get_support()].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUEqXK0yu1F"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print( classification_report(y_test, PipelineToDeploy.predict(X_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUo1Odq3kRD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "# Clusters Profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "* Main variables that define a cluster\n",
        "\n",
        "1.   Using main features from previous classifier\n",
        "2.   And variables we are interested (busines acumen)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-k5S-NKFTMW"
      },
      "source": [
        "main_clusters_variables =  df_feature_importance['Attribute'].to_list() + ['State','RainToday','Cloud3pm']\n",
        "main_clusters_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "source": [
        "df_cluster_profile = X.copy()\n",
        "for col in ['Cloud9am','Cloud3pm']:\n",
        "  df_cluster_profile[col] =df_cluster_profile[col].astype('object')\n",
        "\n",
        "\n",
        "df_cluster_profile = df_cluster_profile.filter(items=main_clusters_variables+['Clusters'],axis=1)\n",
        "\n",
        "num_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(include=['number']).columns.to_list()\n",
        "categorical_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(exclude=['number']).columns.to_list()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nl2EOOwtHdc"
      },
      "source": [
        "df_cluster_profile.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NBCTceJbgRu"
      },
      "source": [
        "## Custom Functions for Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a9x3gPbnwu"
      },
      "source": [
        "* Distribution profile for all clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIqEazT5blBg"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "def PlotClustersDistribution(df,num_var,categorical_var):\n",
        "  for col in num_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(10,5));\n",
        "    sns.kdeplot(data=df, x=col, hue=\"Clusters\",palette='Set2')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "  for col in categorical_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(15,5));\n",
        "    sns.countplot(data=df.sort_values(by=col), hue=col, x=\"Clusters\",palette='Set2')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEGYqOOJcKPK"
      },
      "source": [
        "* Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oxlK-aOcKX3"
      },
      "source": [
        "# def IndividualClusterAnalysis(df_cluster_profile):\n",
        "\n",
        "#   sns.set_style(\"darkgrid\")\n",
        "#   for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "#     df_cluster = df_cluster_profile.query(f\"Clusters == {cluster}\")\n",
        "#     print(f\"=================== Cluster {cluster} ===================\")\n",
        "    \n",
        "#     for col in num_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       plt.figure(figsize=(10,5));\n",
        "#       sns.histplot(data=df_cluster, x=col)\n",
        "#       plt.show();\n",
        "\n",
        "#       iqr = df_cluster[col].quantile([0.25,0.75])\n",
        "#       print(f\"* IQR: {iqr[0.25]} - {iqr[0.75]}\")\n",
        "#       print(\"\\n\")\n",
        "\n",
        "#     for col in categorical_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       try:\n",
        "#         plt.figure(figsize=(10,5));\n",
        "#         sns.countplot(data=df_cluster, x=col)\n",
        "#         freq = df_cluster[col].value_counts()\n",
        "#         plt.show()\n",
        "#       except Exception as e:\n",
        "#         print(e)\n",
        "#       print(\"\\n\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "* Description All Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}' ({CategoryPercentage}%) ; \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOOWOX14vQw"
      },
      "source": [
        "## All Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "DescriptionAllClusters(df_cluster_profile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxpVsX_t9Pwz"
      },
      "source": [
        "PlotClustersDistribution(df=df_cluster_profile,num_var=num_var,categorical_var=categorical_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdkP_KIA4ylt"
      },
      "source": [
        "## Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWpacqZncG29"
      },
      "source": [
        "# IndividualClusterAnalysis(df_cluster_profile)  # maybe remove? analysis above is better"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}