{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Regression Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a regression model to predict tomorrow's rainfall levels, in mm.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Regression model\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install scikit-learn==0.23.2\n",
        "! pip install yellowbrick==1.2\n",
        "! pip install lazypredict==0.2.9\n",
        "\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "# import uuid\n",
        "# file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "# with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "# print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "# print(\"\\n\\n\")\n",
        "# os.remove(f\"{file_name}.txt\")\n",
        "# ! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"update\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CNgZ_TiuM6"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobdGQGZfZG7"
      },
      "source": [
        "* Delete cloned repo and move current directory to /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UACixuaiuM6"
      },
      "source": [
        "%cd /content\n",
        "import os\n",
        "!rm -rf {os.environ['RepoName']}\n",
        "\n",
        "print(f\"\\n * Please refresh session folder to validate that {os.environ['RepoName']} folder was removed from this session.\")\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MKNQXhQiuM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"/content/WalkthroughProject/outputs/datasets/collection/WeatherAustralia.csv\")\n",
        "      .query(\"RainTomorrow == 'Yes'\")  # subset RainTomorrow as Yes\n",
        "      .drop(labels=['RainTomorrow'],axis=1)\n",
        "      .dropna(subset=['RainfallTomorrow'])   # drop missing data from target RainfallTomorrow\n",
        "  )\n",
        "\n",
        "\n",
        "# subset RainTomorrow as 1, label: RainfallTomorrow, features: all other variables\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Regressor Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kD7PZ5kZkBT"
      },
      "source": [
        "## Custom transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A96j6zLKZz7N"
      },
      "source": [
        "  * convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "  * get Get Day, Month, Year, Weekday, IsWeekend from Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tTrXFaWSIU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Convert ['Cloud9am','Cloud3pm'] to categorical\n",
        "class ConvertToCategorical(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variables=None):\n",
        "      if not isinstance(variables, list):\n",
        "          self.variables = [variables]\n",
        "      else:\n",
        "          self.variables = variables\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      for feature in self.variables:\n",
        "          X[feature] = X[feature].astype('object')\n",
        "\n",
        "      return X\n",
        "\n",
        "\n",
        "# Get Day, Month, Year, Weekday, IsWeekend from Date\n",
        "class GetFeaturesFromDate(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, variable=None):\n",
        "      self.variable = variable\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "      return self\n",
        "\n",
        "  def transform(self, X):\n",
        "      X = X.copy()\n",
        "      X[self.variable] = pd.to_datetime(X[self.variable])\n",
        "      X['Day'] = X[self.variable].dt.day\n",
        "      X['Month'] = X[self.variable].dt.month\n",
        "      X['Year'] = X[self.variable].dt.year\n",
        "      X['WeekDay']= X[self.variable].dt.weekday\n",
        "      X['IsWeekend'] = X['WeekDay'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "      return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML Pipeline: DataCleaningFeatEng, and Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "from config import config\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Data Cleaning\n",
        "from feature_engine.imputation import AddMissingIndicator\n",
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import DropMissingData\n",
        "from feature_engine.imputation import CategoricalImputer\n",
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      ### Data Cleaning\n",
        "      (\"ConvertToCategorical\",ConvertToCategorical(variables = ['Cloud9am','Cloud3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"GetFeaturesFromDate\",GetFeaturesFromDate(variable= 'Date')\n",
        "      ),\n",
        "       \n",
        "      (\"AddMissingIndicator\",AddMissingIndicator(variables= ['Sunshine', 'Evaporation', 'Cloud3pm',\n",
        "                                                             'Cloud9am', 'Pressure9am', 'Pressure3pm',\n",
        "                                                             'WindDir9am', 'WindGustDir', 'WindGustSpeed',\n",
        "                                                             'Humidity3pm', 'WindDir3pm', 'Temp3pm',\n",
        "                                                             'RainfallToday', 'RainToday',\n",
        "                                                             'WindSpeed3pm', 'Humidity9am','Temp9am',\n",
        "                                                             'WindSpeed9am', 'MinTemp','MaxTemp'])\n",
        "      ),\n",
        "\n",
        "      (\"DropFeatures\",DropFeatures(features_to_drop = ['Sunshine','Evaporation','Cloud9am','Date'])\n",
        "      ),                                         ##########dont drop sunshine\n",
        "\n",
        "      (\"DropMissingData\",DropMissingData(variables =['RainfallToday', 'RainToday'])\n",
        "      ),\n",
        "\n",
        "      (\"CategoricalImputer\",CategoricalImputer(variables=['WindDir9am', 'WindGustDir', 'WindDir3pm','Cloud3pm'],\n",
        "                                                imputation_method='missing',fill_value='Missing')\n",
        "      ),\n",
        "\n",
        "      (\"MedianImputer\",MeanMedianImputer(imputation_method='median',\n",
        "                                          variables=['Pressure3pm', 'Pressure9am','WindGustSpeed',\n",
        "                                                    'Humidity3pm', 'Temp3pm', 'WindSpeed3pm', 'Humidity9am',\n",
        "                                                    'WindSpeed9am','Temp9am','MaxTemp',\n",
        "                                                     'RainfallToday']\n",
        "                                          )\n",
        "      ),\n",
        "\n",
        "      (\"MeanImputer\",MeanMedianImputer(imputation_method='mean',variables=['MinTemp'])\n",
        "      ),\n",
        "\n",
        "      ### Feature Engineering\n",
        "\n",
        "      (\"Winsorizer_iqr\",Winsorizer(capping_method='iqr',tail='both', fold=3,variables = ['RainfallToday'])\n",
        "      ),\n",
        "\n",
        "\n",
        "      (\"PowerTransformer\",PowerTransformer(variables = ['WindSpeed3pm','Humidity3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"YeoJohnsonTransformer\",YeoJohnsonTransformer(variables=['RainfallToday','WindGustSpeed',\n",
        "                                                                'WindSpeed9am','Humidity9am'])\n",
        "      ),\n",
        "\n",
        "      (\"EqualFrequencyDiscretiser\",EqualFrequencyDiscretiser(q=5,variables = ['Latitude','Longitude' ])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol5\",RareLabelEncoder(tol=0.05, n_categories=2, variables=['WindDir3pm'])\n",
        "      ),\n",
        "\n",
        "      (\"RareLabelEncoder_tol7\",RareLabelEncoder(tol=0.06, n_categories=2, variables=['State'])\n",
        "      ),\n",
        "\n",
        "      (\"CountEncoder\",CountFrequencyEncoder(encoding_method='count',\n",
        "                                            variables = ['Location','WindGustDir','WindDir9am',\n",
        "                                                          'WindDir3pm','State','Cloud3pm',\n",
        "                                                          'RainToday'])\n",
        "      )\n",
        "\n",
        "    ]\n",
        "  )\n",
        "  return pipeline_base\n",
        "\n",
        "\n",
        "def PipelineRegressor():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        " \n",
        "  pipe.steps.append([\n",
        "                     \"scaler\",StandardScaler()\n",
        "                     ])\n",
        "  \n",
        "  pipe.steps.append([\n",
        "                     \"model\",DecisionTreeRegressor(random_state=config.RANDOM_STATE)\n",
        "                     ])\n",
        "  return pipe\n",
        "\n",
        "\n",
        "\n",
        "PipelineRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofMbLiVQkD3Y"
      },
      "source": [
        "# Lazy Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIiC8A3dkPQA"
      },
      "source": [
        "* Transform the data using pipeline, except last step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjKHFaC6kPYW"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline_lazy = Pipeline(PipelineRegressor().steps[:-2])\n",
        "columns_after_data_cleaning_feat_eng = pipeline_lazy.fit_transform(df).columns\n",
        "columns_after_data_cleaning_feat_eng\n",
        "\n",
        "pipeline_lazy = Pipeline(PipelineRegressor().steps[:-1])\n",
        "df_lazy = pipeline_lazy.fit_transform(df)\n",
        "df_lazy = pd.DataFrame(data = df_lazy,\n",
        "                       columns = columns_after_data_cleaning_feat_eng)\n",
        "\n",
        "df_lazy = df_lazy.sample(frac=0.7, random_state=config.RANDOM_STATE)\n",
        "\n",
        "df_lazy.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUWVwx6akLzR"
      },
      "source": [
        "* Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiRtbU31kL84"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_lazy.drop(['RainfallTomorrow'],axis=1),\n",
        "                                    df_lazy['RainfallTomorrow'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBmumDTakHco"
      },
      "source": [
        "* Fit Lazy Predict models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIr5aPEokHiE"
      },
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "reg = LazyRegressor(ignore_warnings=False, predictions=False, random_state=config.RANDOM_STATE,regressors='all')\n",
        "models ,predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "# remove these\n",
        "# 'KernelRidge', 'NuSVR', 'GaussianProcessRegressor','SVR','MLPRegressor'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAf5iLjitx-0"
      },
      "source": [
        "* Check performance summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4uBw1mxnJIO"
      },
      "source": [
        "models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "# Modeling - Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "* Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD6B3CuhiDMT"
      },
      "source": [
        "* Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFzP2iGiIk1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['RainfallTomorrow'],axis=1),\n",
        "                                    df['RainfallTomorrow'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt60YblgiF0y"
      },
      "source": [
        "* Use lazy-predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLkUcA7riJD0"
      },
      "source": [
        "* Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imwlDdi6iJK-"
      },
      "source": [
        "pipeline_regressor = PipelineRegressor()\n",
        "pipeline_regressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "* Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "X = df.copy()\n",
        "\n",
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU1E4IdkyLd"
      },
      "source": [
        "* Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lx5EqalOtv"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk0uyz1lSIW"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJzUxAmkv07"
      },
      "source": [
        "* The goal is to merge cluster labels to our data.\n",
        "  * However, the pipeline dropped rows from ['RainfallToday', 'RainToday'] and had AddMissingIndicatorFlag in the process\n",
        "  * Before merging, we need to adjust it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUY6-_6A-rN"
      },
      "source": [
        "drop_imputer = DropMissingData(variables =['RainfallToday' , 'RainToday'])  #,'RainfallTomorrow','RainTomorrow'])\n",
        "X = drop_imputer.fit_transform(X)\n",
        "\n",
        "na_imputer =  AddMissingIndicator(variables= ['Sunshine', 'Evaporation', 'Cloud3pm',\n",
        "                                           'Cloud9am', 'Pressure9am', 'Pressure3pm',\n",
        "                                            'WindDir9am', 'WindGustDir', 'WindGustSpeed',\n",
        "                                            'Humidity3pm', 'WindDir3pm', 'Temp3pm',\n",
        "                                            #  'RainfallTomorrow','RainTomorrow',  ##########\n",
        "                                            'RainfallToday', 'RainToday',\n",
        "                                            'WindSpeed3pm', 'Humidity9am','Temp9am',\n",
        "                                            'WindSpeed9am', 'MinTemp','MaxTemp'])\n",
        "X = na_imputer.fit_transform(X)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "* We add a column \"Cluster\" to the data and check clusters distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "X['Clusters'] = X['Clusters'].astype('object')\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OhXSh7536Ye"
      },
      "source": [
        "* Clusters don't look to be imbalanced\n",
        "* This is how our data look like from now\n",
        "  * Check the last column: Clusters\n",
        "  * Quick reminder: The data is unprocessed (data cleaning, feat eng); except for the part DropMissingData(variables =['RainfallToday', 'RainToday'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "source": [
        "print(X.shape)\n",
        "X.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "# Regressor Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJPFLoTciOI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "# Classifier to explain cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "* We need to find the most relevant variables, to define each cluster in terms of each relevant variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "source": [
        "df_clf = X.copy() #.sample(frac=0.051, random_state=config.RANDOM_STATE)\n",
        "df_clf['Clusters'] = df_clf['Clusters'].astype('int32')\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "* Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=config.TEST_SIZE,\n",
        "                                    random_state=config.RANDOM_STATE,\n",
        "                                    stratify=df_clf['Clusters']\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "* Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9nz9-UeBl0P"
      },
      "source": [
        "pipeline_clf_cluster['MedianImputer'].imputer_dict_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Cjsn6l5cqF"
      },
      "source": [
        "* Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QPwTqa0OcY"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train,y_train)\n",
        "\n",
        "# do GridCV after"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        " * Evaluate model performance on Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\n",
        "      classification_report(y_train, pipeline_clf_cluster.predict(X_train))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "source": [
        "print(\n",
        "      classification_report(y_test, pipeline_clf_cluster.predict(X_test))\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "* Check main features importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N7Mc3mrAYLq"
      },
      "source": [
        "df_feature_importance = pd.DataFrame(data={\n",
        "    'Attribute': df_clf.columns[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_\n",
        "  })\n",
        "\n",
        "df_feature_importance.sort_values(by='Importance', ascending=False).plot(kind='bar',x='Attribute',y='Importance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQblpQISv0Fd"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "_parameters = {\n",
        "    'model__n_estimators':[50], # [100,200,50],\n",
        "    'model__max_depth': [3] # [None,3,10]\n",
        "}\n",
        "\n",
        "\n",
        "_pipe = GridSearchCV(\n",
        "\t\testimator = pipeline_clf_cluster,\n",
        "\t\tparam_grid = _parameters, \n",
        "\t\tcv=2,n_jobs=-2,verbose=2)\n",
        "_pipe.fit(X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4baRUyFv0IL"
      },
      "source": [
        "PipelineToDeploy = _pipe.best_estimator_\n",
        "PipelineToDeploy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZ0bHjDyrMr"
      },
      "source": [
        "_pipe.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwzpgGwys-x"
      },
      "source": [
        "X_train.columns[PipelineToDeploy['feat_selection'].get_support()].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUEqXK0yu1F"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print( classification_report(y_test, PipelineToDeploy.predict(X_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUo1Odq3kRD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "# Clusters Profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "* Main variables that define a cluster\n",
        "\n",
        "1.   Using main features from previous classifier\n",
        "2.   And variables we are interested (busines acumen)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-k5S-NKFTMW"
      },
      "source": [
        "main_clusters_variables =  df_feature_importance['Attribute'].to_list() + ['State','RainToday','Cloud3pm']\n",
        "main_clusters_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "source": [
        "df_cluster_profile = X.copy()\n",
        "for col in ['Cloud9am','Cloud3pm']:\n",
        "  df_cluster_profile[col] =df_cluster_profile[col].astype('object')\n",
        "\n",
        "\n",
        "df_cluster_profile = df_cluster_profile.filter(items=main_clusters_variables+['Clusters'],axis=1)\n",
        "\n",
        "num_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(include=['number']).columns.to_list()\n",
        "categorical_var = df_cluster_profile.filter(main_clusters_variables,axis=1).select_dtypes(exclude=['number']).columns.to_list()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nl2EOOwtHdc"
      },
      "source": [
        "df_cluster_profile.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NBCTceJbgRu"
      },
      "source": [
        "## Custom Functions for Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35a9x3gPbnwu"
      },
      "source": [
        "* Distribution profile for all clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIqEazT5blBg"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "def PlotClustersDistribution(df,num_var,categorical_var):\n",
        "  for col in num_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(10,5));\n",
        "    sns.kdeplot(data=df, x=col, hue=\"Clusters\",palette='Set2')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "  for col in categorical_var:\n",
        "    print(f\"* {col} distribution per cluster\")\n",
        "    plt.figure(figsize=(15,5));\n",
        "    sns.countplot(data=df.sort_values(by=col), hue=col, x=\"Clusters\",palette='Set2')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEGYqOOJcKPK"
      },
      "source": [
        "* Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oxlK-aOcKX3"
      },
      "source": [
        "# def IndividualClusterAnalysis(df_cluster_profile):\n",
        "\n",
        "#   sns.set_style(\"darkgrid\")\n",
        "#   for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "#     df_cluster = df_cluster_profile.query(f\"Clusters == {cluster}\")\n",
        "#     print(f\"=================== Cluster {cluster} ===================\")\n",
        "    \n",
        "#     for col in num_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       plt.figure(figsize=(10,5));\n",
        "#       sns.histplot(data=df_cluster, x=col)\n",
        "#       plt.show();\n",
        "\n",
        "#       iqr = df_cluster[col].quantile([0.25,0.75])\n",
        "#       print(f\"* IQR: {iqr[0.25]} - {iqr[0.75]}\")\n",
        "#       print(\"\\n\")\n",
        "\n",
        "#     for col in categorical_var:\n",
        "#       print(f\"* {col} distribution for cluster {cluster}\")\n",
        "#       try:\n",
        "#         plt.figure(figsize=(10,5));\n",
        "#         sns.countplot(data=df_cluster, x=col)\n",
        "#         freq = df_cluster[col].value_counts()\n",
        "#         plt.show()\n",
        "#       except Exception as e:\n",
        "#         print(e)\n",
        "#       print(\"\\n\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "* Description All Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}' ({CategoryPercentage}%) ; \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeOOWOX14vQw"
      },
      "source": [
        "## All Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "DescriptionAllClusters(df_cluster_profile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxpVsX_t9Pwz"
      },
      "source": [
        "PlotClustersDistribution(df=df_cluster_profile,num_var=num_var,categorical_var=categorical_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdkP_KIA4ylt"
      },
      "source": [
        "## Individual Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWpacqZncG29"
      },
      "source": [
        "# IndividualClusterAnalysis(df_cluster_profile)  # maybe remove? analysis above is better"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}