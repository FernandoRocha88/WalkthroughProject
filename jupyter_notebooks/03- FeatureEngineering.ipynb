{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoRocha88/WalkthroughProject/blob/main/jupyter_notebooks/03-%20FeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Feature Engineering Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Engineer features for Clf, Reg and Cluster models\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate Train and Test sets with engineered features, both saved under inputs/datasets/feat_eng\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "\n",
        "\n",
        "* Feature Engineering\n",
        "  * xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGT0ZCtwFAFv"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcidnQspZztu"
      },
      "source": [
        "! pip install matplotlib -U\n",
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install ppscore==1.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueUKBo7JpjkL"
      },
      "source": [
        "* Code for restarting the runtime (that will restart colab session, all your variables will be lost)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1anPl4iFEPR"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n",
        "  * Typically the output will be /device:GPU:0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "UserName = getpass('GitHub User Name: ')\n",
        "UserEmail = getpass('GitHub User E-mail: ')\n",
        "RepoName = getpass('GitHub Repository Name: ')\n",
        "UserPwd = getpass('GitHub Account Password: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RStVvDjfTxAk"
      },
      "source": [
        "! git clone https://github.com/{UserName}/{RepoName}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need for this project\n",
        "\n",
        "print(\"\\n\")\n",
        "%cd /content/{RepoName}\n",
        "print(f\"\\n\\n* Current session directory is:  {os.getcwd()}\")\n",
        "print(f\"* You may refresh the session folder to access {RepoName} folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8MWs250vtR"
      },
      "source": [
        "!git config --global user.email {UserEmail}\n",
        "!git config --global user.name {UserName}\n",
        "!git remote rm origin\n",
        "!git remote add origin https://{UserName}:{UserPwd}@github.com/{UserName}/{RepoName}.git\n",
        "\n",
        "print(f\"\\n\\n * The current Colab Session is connected to the following GitHub repo: {UserName}/{RepoName}\")\n",
        "print(\" * You can now push new files to the repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwFQLlmwrl9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZcmA1wG8AdC"
      },
      "source": [
        "### **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dafOBor8OoM"
      },
      "source": [
        "CommitMsg = \"added-cleaned-data\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkyUs70oloW"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      },
      "source": [
        "!git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXBDTg2ouLC"
      },
      "source": [
        "### **Delete** Cloned Repo from current Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_twMc7cefGw"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf {RepoName}\n",
        "print(f\"\\n * Please refresh session folder to validate that {RepoName} folder was removed from this session.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LEJkEZwl2K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"/content/WalkthroughProject/outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-WRjItbiOs6"
      },
      "source": [
        "test_set_path = '/content/WalkthroughProject/outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Quick exploration with Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_SfYdup7oFu"
      },
      "source": [
        "TrainSet['RainTomorrow']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet,minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvwabO0JsmYW"
      },
      "source": [
        "# Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKfCrF-csmYl"
      },
      "source": [
        "* which variables are more correlated with a given set of variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpnOBg5UsmYn"
      },
      "source": [
        "df_corr_spearman = TrainSet.corr(method=\"spearman\")\n",
        "df_corr_pearson = TrainSet.corr(method=\"pearson\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Zy_MglsmYo"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def heatmap_correlation(df_corr,CorrThreshold,NumberOfColumns):\n",
        "\n",
        "  if NumberOfColumns > 1:\n",
        "      mask = np.zeros_like(df_corr, dtype=np.bool)\n",
        "      mask[np.triu_indices_from(mask)] = True\n",
        "      mask[abs(df_corr) < CorrThreshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,8))\n",
        "      ax = sns.heatmap(data=df_corr,annot=True,\n",
        "                       xticklabels=True,yticklabels=True,mask=mask,\n",
        "                       cmap='viridis',annot_kws={\"size\": 8})\n",
        "      plt.ylim(NumberOfColumns,0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "def pairplot_correlation(df,transparency,hue=None):\n",
        "  \n",
        "  if hue == None:\n",
        "    fig = sns.pairplot(data=df,plot_kws={'alpha':transparency})\n",
        "  else:\n",
        "    fig = sns.pairplot(data=df,hue= hue,plot_kws={'alpha':transparency})\n",
        "  \n",
        "  for i, j in zip(*np.triu_indices_from(fig.axes, 1)):\n",
        "      fig.axes[i, j].set_visible(False)\n",
        "  \n",
        "  plt.figure(figsize=(20,8))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUXLq0Ezmsap"
      },
      "source": [
        "* **Correlation Analysis**\n",
        "  * Analyze how the target variable for your ML models are correlated with other variables (features and target)\n",
        "  * Analyze multi colinearity, that is, how the features are correlated among themselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrgabnPsmYp"
      },
      "source": [
        "print(\"Correlation Heatmap - Spearman: evaluates monotonic relationship \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_spearman, CorrThreshold=0.6,NumberOfColumns = len(TrainSet.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5AQ0wCEsmYq"
      },
      "source": [
        "print(\"Correlation Heatmap - Pearson: evaluates the linear relationship between two continuous variables \\n\")\n",
        "heatmap_correlation(df_corr=df_corr_pearson,CorrThreshold=0.6,NumberOfColumns = len(TrainSet.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHid6tTJTsGr"
      },
      "source": [
        "# Power Predictive Score - PPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw9obEodJ3DZ"
      },
      "source": [
        "* Either load PPS analysis or calculate; then preprare for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRUz0MjCVUSG"
      },
      "source": [
        "import ppscore as pps\n",
        "# try:\n",
        "#   pps_matrix_raw = pd.read_csv(\"/content/WalkthroughProject/outputs/data_cleaning/pps_analysis.csv\")\n",
        "# except:\n",
        "pps_matrix_raw = pps.matrix(TrainSet)\n",
        "  # pps_matrix_raw.to_csv(\"/content/WalkthroughProject/outputs/data_cleaning/pps_analysis.csv\",index=False)\n",
        "\n",
        "pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BBWeV0Ke5s"
      },
      "source": [
        "* PPS score distribution\n",
        "* It helps to tell the PPS Threshold for relevant relationships. \n",
        "  * It is suggested that if Q3 (or 75%) is lower than 0.2, a pps greater than 0.2 is a relevant relationship\n",
        "  * If Q3 is greater than 0.2, pps values greater than Q3 are a relevant relationship "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyc5GqjmKMDm"
      },
      "source": [
        "pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfH4PfHSJ_Sx"
      },
      "source": [
        "* Function: Heatmap for PPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bMnEgQV9Tk"
      },
      "source": [
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "def heatmap_pps(df,PPS_Threshold):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < PPS_Threshold] = True\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(20,12))\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                         mask=mask,cmap='rocket_r', annot_kws={\"size\": 7})\n",
        "        \n",
        "        plt.ylim(len(df.columns),0)\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH4isTxFCiC9"
      },
      "source": [
        "print(f\"* PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "      f\"* The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "heatmap_pps(df=pps_matrix,PPS_Threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9zGtSs7WzfZ"
      },
      "source": [
        "* pps heatmap with target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FOfvanfW1SK"
      },
      "source": [
        "def heatmap_pps_target(df,NumberOfColumns):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "  fig, ax = plt.subplots(figsize=(20,8))\n",
        "  ax = sns.heatmap(\n",
        "          df,\n",
        "          xticklabels=True,\n",
        "          yticklabels=True,\n",
        "          annot=True,\n",
        "          cmap='coolwarm',\n",
        "          annot_kws={\"size\": 8})\n",
        "\n",
        "  plt.ylim(NumberOfColumns,0)\n",
        "  plt.show()\n",
        "\n",
        "heatmap_pps_target(df=pps_matrix_raw,NumberOfColumns=df.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZsS4AdFjUYH"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUsyE39kjWrI"
      },
      "source": [
        "* At this stage, there are no missing data in your Train and Test sets.\n",
        "* Now you are looking to engineer, to transform, your variables, so the Machine Learning model will better learn the relationships among the variables (features and lables).\n",
        "  * It is important to run a quick EDA to asess variables distribution shape. Machine Learning models learn better when the distribution is normal. To engineer that, you can use transformers in packages like feature-engine or sklearn.\n",
        "  * You can also use your business acumen and technical expertise to create new variables. For example, imagine if your dataset is about your orange juice company operation. There is a variable called \"revenue\" and other called \"volume\", you divide revenue by volume to know how much money you make per liter of manufactured juice\n",
        "\n",
        "---   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvG1I9mHmNWC"
      },
      "source": [
        "* **Strategy**\n",
        "\n",
        "\n",
        "* **1 - List all variables you are initially interested to engineer, dividing per groups, first numerical variables, then categorical variables**\n",
        "\n",
        "* Numerical\n",
        "  * MaxTemp\n",
        "  * Rainfall\n",
        "  * WindGustSpeed\n",
        "  * WindSpeed9am\n",
        "  * WindSpeed3pm\n",
        "  * Humidity9am\n",
        "  * Humidity3pm\n",
        "  * Temp9am\n",
        "  * Temp3pm\n",
        "  * Latitude\n",
        "  * Longitude\n",
        "  * RainfallTomorrow  (target variable for Reg model)\n",
        "\n",
        "* Categorical\n",
        "  * WindGustDir\n",
        "  * WindDir9am\n",
        "  * WindDir3pm\n",
        "  * Cloud9am\n",
        "  * State\n",
        "  * Location\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWsUzp8IoVxw"
      },
      "source": [
        "\n",
        "* **2 - Consider the following template to help your engineering process**\n",
        "\n",
        "  * 1 - Select variable(s)\n",
        "  * 2 - Select the transformer(s)\n",
        "  * 3 - Create a separate dataframe, for that variable(s)\n",
        "  * 4 - Create engineered variables(s) applying the transformation(s)\n",
        "  * 5 - Assess engineered variables distribution and select most suitable transformation\n",
        "  * 6 - If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqEHqNwyrucq"
      },
      "source": [
        "## Custom functions for engineering numerical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "masShIoqzf2b"
      },
      "source": [
        "from feature_engine import transformation as vt\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser, EqualWidthDiscretiser\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "\n",
        "\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set(style=\"darkgrid\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineering(df,analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feat engineering on numerical variables\n",
        "    to decide which transformation can better transform the distribution shape to\n",
        "    look like more gaussian.\n",
        "    - Once transformed, use a reporting tool, like sweetviz, to evaluate distributions\n",
        "\n",
        "    - Transformers applied include: LogTransformer, ReciprocalTransformer,\n",
        "    PowerTransformer, BoxCoxTransformer, YeoJohnsonTransformer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ### Check analyis type\n",
        "    allowed_types= f\"'numerical', 'outlier', 'discretization', 'count_encoder' or 'outlier_winsorizer'. \"\n",
        "    if analysis_type == None:\n",
        "      raise SystemExit(f\"You should pass analysis_type argument: {allowed_types} \")\n",
        "    if analysis_type not in allowed_types:\n",
        "      raise SystemExit(f\"analysis_type argument should be either {allowed_types}\")\n",
        "\n",
        "\n",
        "    ### Set suffix colummns acording to analysis_type\n",
        "    if analysis_type=='numerical':\n",
        "      list_column_transformers = [\"lte\",\"lt10\",\"rt\", \"pt\",\"bct\",\"yj\"]\n",
        "    \n",
        "    elif analysis_type=='outlier':\n",
        "      list_column_transformers = [\"lt\",\"rt\", \"pt\",\"bct\",\"yj\"]####\n",
        "    \n",
        "    elif analysis_type=='discretization':\n",
        "      list_column_transformers = ['equal_frequency_5intervals',\n",
        "                             'equal_frequency_10intervals',\n",
        "                             'equal_width_5intervals',\n",
        "                             'equal_width_10intervals']\n",
        "    \n",
        "    elif analysis_type=='count_encoder':\n",
        "      list_column_transformers = [\"count_encoder\"]# \"frequency_encoder\"]\n",
        "\n",
        "    elif analysis_type=='outlier_winsorizer':\n",
        "      list_column_transformers = ['gaussian', 'iqr']\n",
        "\n",
        "\n",
        "\n",
        "    df_feat_eng = pd.DataFrame([]) # empty engineered dataframe\n",
        "    for column in df.columns:\n",
        "\n",
        "      ### create additional columns (column_method) to apply the methods\n",
        "      df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "      for method in list_column_transformers:\n",
        "        df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "        \n",
        "      ### Apply transformers in respectives column_transformers\n",
        "      if analysis_type=='numerical':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
        "      \n",
        "      elif analysis_type=='discretization':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_Discretization(df_feat_eng,column)\n",
        "      \n",
        "      elif analysis_type=='outlier_winsorizer':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
        "\n",
        "      elif analysis_type=='count_encoder':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_CountFrequency(df_feat_eng,column)\n",
        "\n",
        "      \n",
        "      # For each variable, assess how the transformations perform\n",
        "      print(f\"* Variable Analyzed: {column}\")\n",
        "      print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "      for col in [column] + list_applied_transformers:\n",
        "        \n",
        "        if analysis_type!='count_encoder':\n",
        "          DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "        \n",
        "        else:\n",
        "          if col == column: \n",
        "            DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "          else:\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "  plt.figure(figsize=(20, 5))\n",
        "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'])\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
        "  plt.show();\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "\n",
        "    fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=ax1) \n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=ax2)\n",
        "    sns.boxplot(x=df[variable],ax=ax3)\n",
        "    \n",
        "    # analysis on outliers\n",
        "    # shapiro analysis\n",
        "    ax1.set_title('Histogram')\n",
        "    ax2.set_title('Probability Plot')\n",
        "    ax3.set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
        "    plt.show();\n",
        "\n",
        "\n",
        "def FeatEngineering_CountFrequency(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "  ###  CountEncoder\n",
        "  try: \n",
        "    encoder= CountFrequencyEncoder(encoding_method='count',variables = [f\"{column}_count_encoder\"])\n",
        "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_count_encoder\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_count_encoder\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### Winsorizer gaussian\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='gaussian', tail='both', fold=3,variables = [f\"{column}_gaussian\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_gaussian\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_gaussian\"],axis=1,inplace=True)\n",
        "\n",
        "  ### Winsorizer iqr\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='iqr', tail='both', fold=3,variables = [f\"{column}_iqr\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_iqr\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def FeatEngineering_Discretization(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### EqualFrequencyDiscretiser\n",
        "  try: \n",
        "    disc= EqualFrequencyDiscretiser(q=5,variables = [f\"{column}_equal_frequency_5intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_frequency_5intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_frequency_5intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualFrequencyDiscretiser\n",
        "  try: \n",
        "    disc= EqualFrequencyDiscretiser(q=10,variables = [f\"{column}_equal_frequency_10intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_frequency_10intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_frequency_10intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualWidthDiscretiser\n",
        "  try: \n",
        "    disc= EqualWidthDiscretiser(bins=5,variables = [f\"{column}_equal_width_5intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_width_5intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_width_5intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualWidthDiscretiser\n",
        "  try: \n",
        "    disc= EqualWidthDiscretiser(bins=10,variables = [f\"{column}_equal_width_10intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_width_10intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_width_10intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "  \n",
        "  \n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### LogTransformer base e\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_lte\"])\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_lte\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_lte\"],axis=1,inplace=True)\n",
        "\n",
        "    ### LogTransformer base 10\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_lt10\"],base='10')\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_lt10\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_lt10\"],axis=1,inplace=True)\n",
        "\n",
        "  ### ReciprocalTransformer\n",
        "  try:\n",
        "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_rt\"])\n",
        "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_rt\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_rt\"],axis=1,inplace=True)\n",
        "\n",
        "  ### PowerTransformer\n",
        "  try:\n",
        "    pt = vt.PowerTransformer(variables = [f\"{column}_pt\"])\n",
        "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_pt\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_pt\"],axis=1,inplace=True)\n",
        "\n",
        "  ### BoxCoxTransformer\n",
        "  try:\n",
        "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_bct\"])\n",
        "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_bct\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_bct\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### YeoJohnsonTransformer\n",
        "  try:\n",
        "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yj\"])\n",
        "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_yj\")\n",
        "  except:\n",
        "        df_feat_eng.drop([f\"{column}_yj\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkGvnFuGocn3"
      },
      "source": [
        "## Template for Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1384nYlNoj5x"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnFS5r6rokB1"
      },
      "source": [
        "variables_engineering = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF0gTJlFqKxs"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TmINQpLqK9X"
      },
      "source": [
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DW23bB9qLcY"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA6nIsm8qLmw"
      },
      "source": [
        "df_engineering = Trainset[variables_engineering].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55WRR4IUqL1r"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMfC-1ZwqL_j"
      },
      "source": [
        "# use custom function\n",
        "df_engineering = ...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKMjoSp7YNLx"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effectie\n",
        "  * xxx\n",
        "  * xxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-JKx14-qMil"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8PTXdovqMtU"
      },
      "source": [
        "TrainSet, TestSet = ...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kMYDKU2yuT"
      },
      "source": [
        "## Numerical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7iQbNn2yuX"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqF5Z5TGrGXB"
      },
      "source": [
        "variables_engineering = ['MinTemp', 'MaxTemp', 'WindGustSpeed', 'WindSpeed9am']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGfq83p02yuY"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkiztpOX2yua"
      },
      "source": [
        "from feature_engine.transformation import (LogTransformer,\n",
        "                                           ReciprocalTransformer,\n",
        "                                           PowerTransformer,\n",
        "                                           BoxCoxTransformer,\n",
        "                                           YeoJohnsonTransformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-pse1Ge2yub"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hnHWkNf2yub"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsN5KNe2yuc"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRdtdLQP2yuc"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='numerical')\n",
        "df_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hDzNkZc6WD1"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effective\n",
        "  * MinTemp - yj\n",
        "  * MaxTemp - not applying any transformation\n",
        "  * WindGustSpeed - bct\n",
        "  * WindSpeed9am - yj\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1Nojcs2yud"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformers to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55pxyEpX2yud"
      },
      "source": [
        "# The steps to apply the transformer to the Train/Test set are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "variable_bct = ['WindGustSpeed']\n",
        "if variable_bct: # if list is not empty, transform\n",
        "  bct = BoxCoxTransformer(variables = variable_bct)\n",
        "  TrainSet = bct.fit_transform(TrainSet)\n",
        "  TestSet = bct.transform(TestSet)\n",
        "\n",
        "variable_yj = ['MinTemp','WindSpeed9am']\n",
        "if variable_yj: # if list is not empty, transform\n",
        "  yj = YeoJohnsonTransformer(variables=variable_yj)\n",
        "  TrainSet = yj.fit_transform(TrainSet)\n",
        "  TestSet = yj.transform(TestSet)\n",
        "\n",
        "\n",
        "  # repeat for other transformers\n",
        "\n",
        "\n",
        "\n",
        "# lte = LogTransformer(variables = )\n",
        "# lt10 = LogTransformer(base='10', variables =)\n",
        "# rt = ReciprocalTransformer(variables =)\n",
        "# pt = PowerTransformer(variables = )\n",
        "# bct = BoxCoxTransformer(variables = )\n",
        "# yjt = YeoJohnsonTransformer(variables = )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eDEgS5IzMLz"
      },
      "source": [
        "## Variable Discretisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcK21l_EzMMA"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NnnEPSTzMMA"
      },
      "source": [
        "variables_engineering= ['Latitude', 'Longitude','Rainfall']\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRQx63aqzMMC"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTMuPAZgzMMC"
      },
      "source": [
        "from feature_engine.discretisation import EqualWidthDiscretiser\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnFSeTeUzMMC"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRh8YaHszMMD"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_RMSr2ZzMMD"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWl9PVgDzMMD"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='discretization')\n",
        "df_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ7huJCqzMME"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effectie\n",
        "  * 'Latitude' - equal frequency 5 intervals\n",
        "  * 'Longitude' - equal frequency 10 intervals\n",
        "  * 'Rainfall' - no transformation applied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTIY09oIzMME"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq9PZmWAzMME"
      },
      "source": [
        "# just reinforcing the transformation we will use\n",
        "from feature_engine.discretisation import EqualWidthDiscretiser\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "\n",
        "\n",
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "\n",
        "\n",
        "variable_equal_freq5 = ['Latitude' ]\n",
        "disc = EqualFrequencyDiscretiser(q=5,variables = variable_equal_freq5)\n",
        "TrainSet = disc.fit_transform(TrainSet)\n",
        "TestSet = disc.transform(TestSet)\n",
        "\n",
        "\n",
        "variable_equal_freq10 = ['Longitude' ]\n",
        "disc = EqualFrequencyDiscretiser(q=10,variables = variable_equal_freq10)\n",
        "TrainSet = disc.fit_transform(TrainSet)\n",
        "TestSet = disc.transform(TestSet)\n",
        "\n",
        "\n",
        "# EqualFrequencyDiscretiser: 'q' argument is the number of intervals\n",
        "# EqualWidthDiscretiser: 'bins' argument is the number of intervals\n",
        "\n",
        "# disc= EqualWidthDiscretiser(bins=????,variables = )\n",
        "# disc = EqualFrequencyDiscretiser(q=???? ,variables = )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "295Hfub2Y8oL"
      },
      "source": [
        "## Categorical Enconding - RareLabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf3mb9ooY8oW"
      },
      "source": [
        "* Step 1: Select variable and describe distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7aSQgqvY8oX"
      },
      "source": [
        "variables_engineering = ['Cloud9am', 'State']\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZn4EUfYY8oY"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaYpfd-FY8oZ"
      },
      "source": [
        "from feature_engine.encoding import RareLabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFOCNMUpY8ob"
      },
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ69-5seY8ob"
      },
      "source": [
        "from feature_engine.encoding import RareLabelEncoder\n",
        "list_tol = [0.01, 0.03,0.05,0.08,0.1,0.12]\n",
        "list_tol = [0.07,0.08]\n",
        "\n",
        "\n",
        "for variable_engi in variables_engineering:\n",
        "  df_engineering = TrainSet[[variable_engi]].copy()\n",
        "\n",
        "  for tol in list_tol:\n",
        "    df_engineering[f\"{variable_engi}_rare_tol {str(tol)}\"] = df_engineering[[variable_engi]]\n",
        "    \n",
        "    \n",
        "    encoder = RareLabelEncoder(tol=tol, n_categories=2, variables=[f\"{variable_engi}_rare_tol {str(tol)}\"],\n",
        "                              replace_with='Rare')\n",
        "    df_engineering = encoder.fit_transform(df_engineering)\n",
        "  \n",
        "\n",
        "  for col in df_engineering.columns: DiagnosticPlots_Categories(df_engineering, col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXGiqI9cY8oc"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effectie\n",
        "  * Clou9am - 4% tol\n",
        "  * State - 7% tol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejj2_halY8oc"
      },
      "source": [
        "* Step 4: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFCPCDpIY8od"
      },
      "source": [
        "# just reinforcing the encoder we will use\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "\n",
        "# the steps are: \n",
        "# 1 - select given tolerance and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "variable_rare= ['clou9am']\n",
        "encoder = RareLabelEncoder(tol=0.04, n_categories=2, variables=variable_rare])\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "variable_rare= ['State']\n",
        "encoder = RareLabelEncoder(tol=0.08, n_categories=2, variables=variable_rare])\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GT8e9W_onJj"
      },
      "source": [
        "## Categorical Enconding - Count: replaces categories by the count of observations per category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwG_KMpOonJu"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ii9nXtJonJv"
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "variables_engineering = TrainSet.select_dtypes(exclude=numerics ).columns\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzOOSSzsonJw"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvQf2rkoonJx"
      },
      "source": [
        "from feature_engine.encoding import CountFrequencyEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t32melBMonJy"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJJBWQlconJy"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mk058LZonJz"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKEuWXrtonJz"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='count_encoder')\n",
        "df_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfAiPd8DonJz"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effective\n",
        "  * 'Location'\n",
        "  * 'WindGustDir'\n",
        "  * 'WindDir9am'\n",
        "  * 'WindDir3pm'\n",
        "  * 'Cloud9am'\n",
        "  * 'RainToday'\n",
        "  * 'RainTomorrow'\n",
        "  * 'State'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM2xZ-YTonJ0"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV4pTKpHonJ0"
      },
      "source": [
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "variable_count = variables_engineering.to_list()\n",
        "if variable_count: # if list is not empty, transform\n",
        "  encoder = CountFrequencyEncoder(encoding_method='count',variables = variable_count)\n",
        "  TrainSet = encoder.fit_transform(TrainSet)\n",
        "  TestSet = encoder.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkyO7KsuyG5A"
      },
      "source": [
        "## Handle Outliers(Winsorizer:caps maximum and/or minimum values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l4eVnHNyG5E"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB9PqsbGyffD"
      },
      "source": [
        "* **Quick reminder: The variable(s) has(ve) to numerical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-dEF_P4yG5E"
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "variables_engineering = TrainSet.select_dtypes(include=numerics ).columns\n",
        "variables_engineering = ['Location','Cloud9am','Humidity3pm','Humidity9am']\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1-fcQnUyG5H"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOkGuVWayG5I"
      },
      "source": [
        "from feature_engine.outliers import Winsorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUA_uA1_yG5I"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrplTb39yG5I"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQqqdvd3yG5J"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZXMUZgEyG5J"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='outlier_winsorizer')\n",
        "df_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5TyFpY1yG5J"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effectie\n",
        "  * 'Location - iqr\n",
        "  * 'Cloud9am' - apply no transformation\n",
        "  * 'Humidity3pm' - apply no transformation\n",
        "  * 'Humidity9am'- gaussian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpgHsRqyG5K"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6_oTrsyG5K"
      },
      "source": [
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "\n",
        "### Winsorizer Gaussian\n",
        "variable_out_gaussian = ['Humidity9am']\n",
        "if variable_out_gaussian: # if list is not empty, transform\n",
        "  out_transf =Winsorizer(capping_method='gaussian',tail='both', fold=3,variables = variable_out_gaussian)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n",
        "\n",
        "### Winsorizer IQR\n",
        "variable_out_iqr = ['Location']\n",
        "if variable_out_iqr: # if list is not empty, transform\n",
        "  out_transf =Winsorizer(capping_method='iqr',tail='both', fold=3,variables = variable_out_iqr)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWLkKtldc06R"
      },
      "source": [
        "## Handle Outliers (OutlierTrimmer: removes observations with outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yrNJys7c06f"
      },
      "source": [
        "* Step 1: Select variable(s) and describe distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wOykNOZc06g"
      },
      "source": [
        "* **Quick reminder: The variable(s) has(ve) to numerical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmntMproc06h"
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "variables_engineering = TrainSet.select_dtypes(include=numerics ).columns\n",
        "variables_engineering= ['Rainfall']\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvY3FNiMc06h"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AvCHx-Wc06i"
      },
      "source": [
        "from feature_engine.outliers import OutlierTrimmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_DI2ypzc06i"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU29rcVpc06i"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDSsOB5Fc06j"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEM2Xkioc06j"
      },
      "source": [
        "# df_engineering = FeatureEngineering(df=df_engineering,analysis_type='outlier_trimmer')\n",
        "# df_engineering\n",
        "\n",
        "# iterate on each variable\n",
        "# create 3 different datafraes, one will contain variable as it is, second will apply transformation with gaussian, and last with IQR\n",
        "# apply transformation on each dataframe\n",
        "# compare row loss on second and third dataframe\n",
        "# plot numerical diagnostic function\n",
        "\n",
        "\n",
        "def LostDueToTrimmer(df_initial,df_trimmed):\n",
        "  lost_percentage = round(100- len(df_trimmed) / len(df_initial) *100,2)\n",
        "\n",
        "  print(f\"* If I apply this transformation, \"\n",
        "        f\"I will lose {lost_percentage}% of the dataset, or {len(df_initial)-len(df_trimmed)} rows. \\n\"\n",
        "        f\"* Dataset rows before transformation: {len(df_initial)} \\n\"\n",
        "        f\"* Dataset rows after transformation: {len(df_trimmed)} \\n\")\n",
        "\n",
        "\n",
        "for col in df_engineering.columns:\n",
        "  df_initial = df_engineering[[col]].copy()\n",
        "  df_gaussian = df_initial.copy()\n",
        "  df_gaussian.rename(mapper={col:f\"{col}_gaussian\"},inplace=True,axis=1)\n",
        "  df_iqr = df_initial.copy()\n",
        "  df_iqr.rename(mapper={col:f\"{col}_iqr\"},inplace=True,axis=1)\n",
        "\n",
        "  DiagnosticPlots_Numerical(df_initial, col)\n",
        "\n",
        "  out_transf =OutlierTrimmer(capping_method='gaussian', tail='both', fold=3,variables = [f\"{col}_gaussian\"])\n",
        "  df_gaussian = out_transf.fit_transform(df_gaussian)\n",
        "  DiagnosticPlots_Numerical(df_gaussian, f\"{col}_gaussian\")\n",
        "  LostDueToTrimmer(df_initial=df_initial,df_trimmed=df_gaussian)\n",
        "  \n",
        "\n",
        "  out_transf=OutlierTrimmer(capping_method='iqr', tail='both', fold=3,variables = [f\"{col}_iqr\"])\n",
        "  df_iqr = out_transf.fit_transform(df_iqr)\n",
        "  DiagnosticPlots_Numerical(df_iqr, f\"{col}_iqr\")\n",
        "  LostDueToTrimmer(df_initial=df_initial,df_trimmed=df_iqr)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkw3jTOc06j"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effectie\n",
        "  * xxx\n",
        "  * xxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiIqgTarc06j"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNK0qWjc06k"
      },
      "source": [
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "\n",
        "\n",
        "### OutlierTrimmer Gaussian\n",
        "variable_out_gaussian = []\n",
        "if variable_out_gaussian: # if list is not empty, transform\n",
        "  out_transf =OutlierTrimmer(capping_method='gaussian', tail='both', fold=3,variables = variable_out_gaussian)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n",
        "\n",
        "### OutlierTrimmer IQR\n",
        "variable_out_iqr = ['Rainfall']\n",
        "if variable_out_iqr: # if list is not empty, transform\n",
        "  out_transf=OutlierTrimmer(capping_method='iqr', tail='both', fold=3,variables=variable_out_iqr)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD--GI5Jvp8h"
      },
      "source": [
        "# Save feature engineered data: Train/Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNWONrYwu6d"
      },
      "source": [
        "# TrainSet.to_csv(\"/content/WalkthroughProject/inputs/datasets/cleaned/TrainSetCleaned.csv\",index=False)\n",
        "# TestSet.to_csv(\"/content/WalkthroughProject/inputs/datasets/cleaned/TestSetCleaned.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMucnZHuD3CP"
      },
      "source": [
        "* You may now go to \"Push generated/new files from this session to GitHub Repo\" section and push these files to the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnS7v_HVb1db"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}